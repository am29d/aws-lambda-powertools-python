{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A suite of utilities for AWS Lambda functions to ease adopting best practices such as tracing, structured logging, custom metrics, and more. Looking for a quick run through of the core utilities? Check out this detailed blog post with a practical example. Install Powertools is available in PyPi. You can use your favourite dependency management tool to install it poetry : poetry add aws-lambda-powertools pip : pip install aws-lambda-powertools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python Lambda Layer Powertools is also available as a Lambda Layer. It is distributed via the AWS Serverless Application Repository (SAR) . We have two layers available, one with core dependencies aws-lambda-powertools-python-layer and one with extras aws-lambda-powertools-python-layer-extras such as pydantic which is required for the parser. Note Extras layer does not support Python 3.6 runtime. This layer is also includes all extra dependencies and is 22.4MB zipped, ~155MB unzipped. App ARN aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. template.yml 1 2 3 4 5 6 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.9.0 # change to latest semantic version available in SAR This will add a nested app stack with an output parameter LayerVersionArn , that you can reference inside your Lambda function definition: 1 2 Layers : - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Here is the list of IAM permissions that you need to add to your deployment IAM role to use the layer: template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Version : '2012-10-17' Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.6.0/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccoundId}:layer:aws-lambda-powertools-python-layer* Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. You can fetch the available versions via the API with: 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer Features Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logging Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Bring your own middleware Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Typing Static typing classes to speedup development in your IDE Batch Handle partial failures for AWS SQS batch processing Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers Environment variables Info Explicit parameters take precedence over environment variables. Environment variables used across suite of utilities. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO Debug mode As a best practice, AWS Lambda Powertools logging statements are suppressed. If necessary, you can enable debugging using set_package_logger : app.py 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger () Tenets AWS Lambda only \u2013 We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices \u2013 The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean \u2013 Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility \u2013 New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community \u2013 We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic \u2013 Utilities follow programming language idioms and language-specific best practices. * Core utilities are Tracer, Logger and Metrics. Optional utilities may vary across languages.","title":"Homepage"},{"location":"#install","text":"Powertools is available in PyPi. You can use your favourite dependency management tool to install it poetry : poetry add aws-lambda-powertools pip : pip install aws-lambda-powertools Quick hello world example using SAM CLI 1 sam init --location https://github.com/aws-samples/cookiecutter-aws-sam-python","title":"Install"},{"location":"#lambda-layer","text":"Powertools is also available as a Lambda Layer. It is distributed via the AWS Serverless Application Repository (SAR) . We have two layers available, one with core dependencies aws-lambda-powertools-python-layer and one with extras aws-lambda-powertools-python-layer-extras such as pydantic which is required for the parser. Note Extras layer does not support Python 3.6 runtime. This layer is also includes all extra dependencies and is 22.4MB zipped, ~155MB unzipped. App ARN aws-lambda-powertools-python-layer arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer aws-lambda-powertools-python-layer-extras arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer-extras If using SAM, you can include this SAR App as part of your shared Layers stack, and lock to a specific semantic version. Once deployed, it'll be available across the account this is deployed to. template.yml 1 2 3 4 5 6 AwsLambdaPowertoolsPythonLayer : Type : AWS::Serverless::Application Properties : Location : ApplicationId : arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer SemanticVersion : 1.9.0 # change to latest semantic version available in SAR This will add a nested app stack with an output parameter LayerVersionArn , that you can reference inside your Lambda function definition: 1 2 Layers : - !GetAtt AwsLambdaPowertoolsPythonLayer.Outputs.LayerVersionArn Here is the list of IAM permissions that you need to add to your deployment IAM role to use the layer: template.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Version : '2012-10-17' Statement : - Sid : CloudFormationTransform Effect : Allow Action : cloudformation:CreateChangeSet Resource : - arn:aws:cloudformation:us-east-1:aws:transform/Serverless-2016-10-31 - Sid : GetCfnTemplate Effect : Allow Action : - serverlessrepo:CreateCloudFormationTemplate - serverlessrepo:GetCloudFormationTemplate Resource : # this is arn of the powertools SAR app - arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer - Sid : S3AccessLayer Effect : Allow Action : - s3:GetObject Resource : # AWS publishes to an external S3 bucket locked down to your account ID # The below example is us publishing lambda powertools # Bucket: awsserverlessrepo-changesets-plntc6bfnfj # Key: *****/arn:aws:serverlessrepo:eu-west-1:057560766410:applications-aws-lambda-powertools-python-layer-versions-1.6.0/aeeccf50-****-****-****-********* - arn:aws:s3:::awsserverlessrepo-changesets-*/* - Sid : GetLayerVersion Effect : Allow Action : - lambda:PublishLayerVersion - lambda:GetLayerVersion Resource : - !Sub arn:aws:lambda:${AWS::Region}:${AWS::AccoundId}:layer:aws-lambda-powertools-python-layer* Credits to mwarkentin for providing the scoped down IAM permissions. The region and the account id for CloudFormationTransform and GetCfnTemplate are fixed. You can fetch the available versions via the API with: 1 2 aws serverlessrepo list-application-versions \\ --application-id arn:aws:serverlessrepo:eu-west-1:057560766410:applications/aws-lambda-powertools-python-layer","title":"Lambda Layer"},{"location":"#features","text":"Utility Description Tracing Decorators and utilities to trace Lambda function handlers, and both synchronous and asynchronous functions Logging Structured logging made easier, and decorator to enrich structured logging with key Lambda context details Metrics Custom Metrics created asynchronously via CloudWatch Embedded Metric Format (EMF) Bring your own middleware Decorator factory to create your own middleware to run logic before, and after each Lambda invocation Parameters Retrieve parameter values from AWS Systems Manager Parameter Store, AWS Secrets Manager, or Amazon DynamoDB, and cache them for a specific amount of time Typing Static typing classes to speedup development in your IDE Batch Handle partial failures for AWS SQS batch processing Validation JSON Schema validator for inbound events and responses Event source data classes Data classes describing the schema of common Lambda event triggers","title":"Features"},{"location":"#environment-variables","text":"Info Explicit parameters take precedence over environment variables. Environment variables used across suite of utilities. Environment variable Description Utility Default POWERTOOLS_SERVICE_NAME Sets service name used for tracing namespace, metrics dimension and structured logging All \"service_undefined\" POWERTOOLS_METRICS_NAMESPACE Sets namespace used for metrics Metrics None POWERTOOLS_TRACE_DISABLED Disables tracing Tracing false POWERTOOLS_TRACER_CAPTURE_RESPONSE Captures Lambda or method return as metadata. Tracing true POWERTOOLS_TRACER_CAPTURE_ERROR Captures Lambda or method exception as metadata. Tracing true POWERTOOLS_TRACE_MIDDLEWARES Creates sub-segment for each custom middleware Middleware factory false POWERTOOLS_LOGGER_LOG_EVENT Logs incoming event Logging false POWERTOOLS_LOGGER_SAMPLE_RATE Debug log sampling Logging 0 POWERTOOLS_LOG_DEDUPLICATION_DISABLED Disables log deduplication filter protection to use Pytest Live Log feature Logging false LOG_LEVEL Sets logging level Logging INFO","title":"Environment variables"},{"location":"#debug-mode","text":"As a best practice, AWS Lambda Powertools logging statements are suppressed. If necessary, you can enable debugging using set_package_logger : app.py 1 2 3 from aws_lambda_powertools.logging.logger import set_package_logger set_package_logger ()","title":"Debug mode"},{"location":"#tenets","text":"AWS Lambda only \u2013 We optimise for AWS Lambda function environments and supported runtimes only. Utilities might work with web frameworks and non-Lambda environments, though they are not officially supported. Eases the adoption of best practices \u2013 The main priority of the utilities is to facilitate best practices adoption, as defined in the AWS Well-Architected Serverless Lens; all other functionality is optional. Keep it lean \u2013 Additional dependencies are carefully considered for security and ease of maintenance, and prevent negatively impacting startup time. We strive for backwards compatibility \u2013 New features and changes should keep backwards compatibility. If a breaking change cannot be avoided, the deprecation and migration process should be clearly defined. We work backwards from the community \u2013 We aim to strike a balance of what would work best for 80% of customers. Emerging practices are considered and discussed via Requests for Comment (RFCs) Idiomatic \u2013 Utilities follow programming language idioms and language-specific best practices. * Core utilities are Tracer, Logger and Metrics. Optional utilities may vary across languages.","title":"Tenets"},{"location":"core/logger/","text":"Logger provides an opinionated logger with output structured as JSON. Key features Capture key fields from Lambda context, cold start and structures logging output as JSON Log Lambda event when instructed (disabled by default) Enable via POWERTOOLS_LOGGER_LOG_EVENT=\"true\" or explicitly via decorator param Log sampling enables DEBUG log level for a percentage of requests (disabled by default) Enable via POWERTOOLS_LOGGER_SAMPLE_RATE=0.1 , ranges from 0 to 1, where 0.1 is 10% and 1 is 100% Append additional keys to structured log at any point in time Initialization Set LOG_LEVEL env var as a start - Here is an example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO By default, Logger uses INFO log level. You can either change log level via level param or via env var. You can also explicitly set a service name via service param or via POWERTOOLS_SERVICE_NAME env var. This sets service key that will be present across all log statements. app.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger # POWERTOOLS_SERVICE_NAME defined logger = Logger () # Explicit definition Logger ( service = \"payment\" , level = \"INFO\" ) Standard structured keys Your Logger will include the following keys to your structured logging, by default: Key Type Example Description timestamp str \"2020-05-24 18:17:33,774\" Timestamp of actual log statement level str \"INFO\" Logging level location str \"collect.handler:1\" Source code location where statement was executed service str \"payment\" Service name defined. \"service_undefined\" will be used if unknown sampling_rate int 0.1 Debug logging sampling rate in percentage e.g. 10% in this case message any \"Collecting payment\" Log statement value. Unserializable JSON values will be casted to string xray_trace_id str \"1-5759e988-bd862e3fe1be46a994272793\" X-Ray Trace ID when Lambda function has enabled Tracing Capturing Lambda context info You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : \"Collecting payment\" }, { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:15\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" :{ \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" } } When used, this will include the following keys: Key Type Example cold_start bool false function_name str \"example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_memory_size int 128 function_arn str \"arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_request_id str \"899856cb-83d1-40d7-8611-9e78f15f32f4\" You can also explicitly log any incoming event using log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ... Appending additional keys You can append additional keys using either mechanism: Persist new keys across all future log messages via structure_logs Add additional keys on a per log message basis via extra parameter structure_logs You can append your own keys to your existing Logger via structure_logs with append param. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): order_id = event . get ( \"order_id\" ) logger . structure_logs ( append = True , order_id = order_id ) logger . info ( \"Collecting payment\" ) ... CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"order_id\" : \"order_id_value\" , \"message\" : \"Collecting payment\" } Note: Logger will automatically reject any key with a None value. If you conditionally add keys depending on the payload, you can use the highlighted line above as an example. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the logger. extra parameter Extra parameter is available for all log levels, as implemented in the standard logging library. It accepts any dictionary, and it'll be added as part of the root structure of the logs. extra_parameter.py 1 2 3 4 5 logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Hello\" , extra = fields ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2021-01-12 14:08:12,357\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"request_id\" : \"1123\" , \"message\" : \"Collecting payment\" } Reusing Logger across your code Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 9 10 # POWERTOOLS_SERVICE_NAME: \"payment\" import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): shared . inject_payment_id ( event ) logger . structure_logs ( append = True , order_id = event [ \"order_id\" ]) ... shared.py 1 2 3 4 5 6 7 # POWERTOOLS_SERVICE_NAME: \"payment\" from aws_lambda_powertools import Logger logger = Logger ( child = True ) def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event [ \"payment_id\" ]) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Any changes in the parent and child Loggers will be propagated among them. If you ever forget to use child param, we will return an existing Logger with the same service name. Note Child loggers will be named after the following convention service.filename . Sampling debug logs Sampling allows you to set your Logger Log Level as DEBUG based on a percentage of your concurrent/cold start invocations. You can set a sampling value of 0.0 to 1 (100%) using either sample_rate parameter or POWERTOOLS_LOGGER_SAMPLE_RATE env var. This is useful when you want to troubleshoot an issue, say a sudden increase in concurrency, and you might not have enough information in your logs as Logger log level was understandably set as INFO. Sampling decision happens at the Logger class initialization, which only happens during a cold start. This means sampling may happen significantly more or less than you expect if you have a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling on every invocation, then please open a feature request. collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( sample_rate = 0.1 , level = \"INFO\" ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) if \"order_id\" in event : logger . info ( \"Collecting payment\" ) ... CloudWatch Logs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" } { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Collecting payment\" } Migrating from other Loggers If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions . service parameter Service is what defines what the function is responsible for, or part of (e.g payment service), and the name of the Logger. For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment . inheriting Loggers Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 . For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, thus no message being logged to standard output. This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set. overriding Log records You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id , and datefmt lambda_handler.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # override default values for location and timestamp format logger = Logger ( stream = stdout , location = \"[ %(funcName)s ] %(module)s \" , datefmt = \"fake-datefmt\" ) # suppress location key logger = Logger ( stream = stdout , location = None ) Alternatively, you can also change the order of the following log record keys via the log_record_order parameter: level , location , message , xray_trace_id , and timestamp lambda_handler.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( stream = stdout , log_record_order = [ \"message\" ]) # Default key sorting order logger = Logger ( stream = stdout , log_record_order = [ \"level\" , \"location\" , \"message\" , \"timestamp\" ]) Some keys cannot be supressed in the Log records: sampling_rate is part of the specification and cannot be supressed; xray_trace_id is supressed automatically if X-Ray is not enabled in the Lambda function, and added automatically if it is. logging exceptions When logging exceptions, Logger will add a new key named exception , and will serialize the full traceback as a string. logging_an_exception.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"<module>:4\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2020-08-28 18:11:38,886\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" } Testing your code When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_handler , lambda_context ): test_event = { 'test' : 'event' } lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated pytest live log feature Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured). FAQ How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between structure_log and extra ? Keys added with structure_log will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Example - Persisting payment_id not request_id lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . structure_logs ( append = True , payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Note that payment_id remains in both log messages while booking_id is only available in the first message. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:5\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:6\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" }","title":"Logger"},{"location":"core/logger/#initialization","text":"Set LOG_LEVEL env var as a start - Here is an example using AWS Serverless Application Model (SAM) template.yaml 1 2 3 4 5 6 7 8 9 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Environment : Variables : LOG_LEVEL : INFO By default, Logger uses INFO log level. You can either change log level via level param or via env var. You can also explicitly set a service name via service param or via POWERTOOLS_SERVICE_NAME env var. This sets service key that will be present across all log statements. app.py 1 2 3 4 5 6 from aws_lambda_powertools import Logger # POWERTOOLS_SERVICE_NAME defined logger = Logger () # Explicit definition Logger ( service = \"payment\" , level = \"INFO\" )","title":"Initialization"},{"location":"core/logger/#standard-structured-keys","text":"Your Logger will include the following keys to your structured logging, by default: Key Type Example Description timestamp str \"2020-05-24 18:17:33,774\" Timestamp of actual log statement level str \"INFO\" Logging level location str \"collect.handler:1\" Source code location where statement was executed service str \"payment\" Service name defined. \"service_undefined\" will be used if unknown sampling_rate int 0.1 Debug logging sampling rate in percentage e.g. 10% in this case message any \"Collecting payment\" Log statement value. Unserializable JSON values will be casted to string xray_trace_id str \"1-5759e988-bd862e3fe1be46a994272793\" X-Ray Trace ID when Lambda function has enabled Tracing","title":"Standard structured keys"},{"location":"core/logger/#capturing-lambda-context-info","text":"You can enrich your structured logs with key Lambda context information via inject_lambda_context . collect.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context def handler ( event , context ): logger . info ( \"Collecting payment\" ) ... # You can log entire objects too logger . info ({ \"operation\" : \"collect_payment\" , \"charge_id\" : event [ 'charge_id' ] }) ... Example CloudWatch Logs excerpt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" : \"Collecting payment\" }, { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:15\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.0 , \"message\" :{ \"operation\" : \"collect_payment\" , \"charge_id\" : \"ch_AZFlk2345C0\" } } When used, this will include the following keys: Key Type Example cold_start bool false function_name str \"example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_memory_size int 128 function_arn str \"arn:aws:lambda:eu-west-1:012345678910:function:example-powertools-HelloWorldFunction-1P1Z6B39FLU73\" function_request_id str \"899856cb-83d1-40d7-8611-9e78f15f32f4\" You can also explicitly log any incoming event using log_event param or via POWERTOOLS_LOGGER_LOG_EVENT env var. Warning This is disabled by default to prevent sensitive info being logged. log_handler_event.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () @logger . inject_lambda_context ( log_event = True ) def handler ( event , context ): ...","title":"Capturing Lambda context info"},{"location":"core/logger/#appending-additional-keys","text":"You can append additional keys using either mechanism: Persist new keys across all future log messages via structure_logs Add additional keys on a per log message basis via extra parameter","title":"Appending additional keys"},{"location":"core/logger/#structure_logs","text":"You can append your own keys to your existing Logger via structure_logs with append param. collect.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): order_id = event . get ( \"order_id\" ) logger . structure_logs ( append = True , order_id = order_id ) logger . info ( \"Collecting payment\" ) ... CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"order_id\" : \"order_id_value\" , \"message\" : \"Collecting payment\" } Note: Logger will automatically reject any key with a None value. If you conditionally add keys depending on the payload, you can use the highlighted line above as an example. This example will add order_id if its value is not empty, and in subsequent invocations where order_id might not be present it'll remove it from the logger.","title":"structure_logs"},{"location":"core/logger/#extra-parameter","text":"Extra parameter is available for all log levels, as implemented in the standard logging library. It accepts any dictionary, and it'll be added as part of the root structure of the logs. extra_parameter.py 1 2 3 4 5 logger = Logger ( service = \"payment\" ) fields = { \"request_id\" : \"1123\" } logger . info ( \"Hello\" , extra = fields ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"timestamp\" : \"2021-01-12 14:08:12,357\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"request_id\" : \"1123\" , \"message\" : \"Collecting payment\" }","title":"extra parameter"},{"location":"core/logger/#reusing-logger-across-your-code","text":"Logger supports inheritance via child parameter. This allows you to create multiple Loggers across your code base, and propagate changes such as new keys to all Loggers. collect.py 1 2 3 4 5 6 7 8 9 10 # POWERTOOLS_SERVICE_NAME: \"payment\" import shared # Creates a child logger named \"payment.shared\" from aws_lambda_powertools import Logger logger = Logger () def handler ( event , context ): shared . inject_payment_id ( event ) logger . structure_logs ( append = True , order_id = event [ \"order_id\" ]) ... shared.py 1 2 3 4 5 6 7 # POWERTOOLS_SERVICE_NAME: \"payment\" from aws_lambda_powertools import Logger logger = Logger ( child = True ) def inject_payment_id ( event ): logger . structure_logs ( append = True , payment_id = event [ \"payment_id\" ]) In this example, Logger will create a parent logger named payment and a child logger named payment.shared . Any changes in the parent and child Loggers will be propagated among them. If you ever forget to use child param, we will return an existing Logger with the same service name. Note Child loggers will be named after the following convention service.filename .","title":"Reusing Logger across your code"},{"location":"core/logger/#sampling-debug-logs","text":"Sampling allows you to set your Logger Log Level as DEBUG based on a percentage of your concurrent/cold start invocations. You can set a sampling value of 0.0 to 1 (100%) using either sample_rate parameter or POWERTOOLS_LOGGER_SAMPLE_RATE env var. This is useful when you want to troubleshoot an issue, say a sudden increase in concurrency, and you might not have enough information in your logs as Logger log level was understandably set as INFO. Sampling decision happens at the Logger class initialization, which only happens during a cold start. This means sampling may happen significantly more or less than you expect if you have a steady low number of invocations and thus few cold starts. Note If you want Logger to calculate sampling on every invocation, then please open a feature request. collect.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools import Logger # Sample 10% of debug logs e.g. 0.1 logger = Logger ( sample_rate = 0.1 , level = \"INFO\" ) def handler ( event , context ): logger . debug ( \"Verifying whether order_id is present\" ) if \"order_id\" in event : logger . info ( \"Collecting payment\" ) ... CloudWatch Logs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"DEBUG\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Verifying whether order_id is present\" } { \"timestamp\" : \"2020-05-24 18:17:33,774\" , \"level\" : \"INFO\" , \"location\" : \"collect.handler:1\" , \"service\" : \"payment\" , \"lambda_function_name\" : \"test\" , \"lambda_function_memory_size\" : 128 , \"lambda_function_arn\" : \"arn:aws:lambda:eu-west-1:12345678910:function:test\" , \"lambda_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , \"cold_start\" : true , \"sampling_rate\" : 0.1 , \"message\" : \"Collecting payment\" }","title":"Sampling debug logs"},{"location":"core/logger/#migrating-from-other-loggers","text":"If you're migrating from other Loggers, there are few key points to be aware of: Service parameter , Inheriting Loggers , Overriding Log records , and Logging exceptions .","title":"Migrating from other Loggers"},{"location":"core/logger/#service-parameter","text":"Service is what defines what the function is responsible for, or part of (e.g payment service), and the name of the Logger. For Logger, the service is the logging key customers can use to search log operations for one or more functions - For example, search for all errors, or messages like X, where service is payment .","title":"service parameter"},{"location":"core/logger/#inheriting-loggers","text":"Python Logging hierarchy happens via the dot notation: service , service.child , service.child_2 . For inheritance, Logger uses a child=True parameter along with service being the same value across Loggers. For child Loggers, we introspect the name of your module where Logger(child=True, service=\"name\") is called, and we name your Logger as {service}.{filename} . A common issue when migrating from other Loggers is that service might be defined in the parent Logger (no child param), and not defined in the child Logger: incorrect_logger_inheritance.py 1 2 3 4 5 6 7 8 9 10 import my_module from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) ... # my_module.py from aws_lambda_powertools import Logger logger = Logger ( child = True ) In this case, Logger will register a Logger named payment , and a Logger named service_undefined . The latter isn't inheriting from the parent, and will have no handler, thus no message being logged to standard output. This can be fixed by either ensuring both has the service value as payment , or simply use the environment variable POWERTOOLS_SERVICE_NAME to ensure service value will be the same across all Loggers when not explicitly set.","title":"inheriting Loggers"},{"location":"core/logger/#overriding-log-records","text":"You might want to continue to use the same date formatting style, or override location to display the package.function_name:line_number as you previously had. Logger allows you to either change the format or suppress the following keys altogether at the initialization: location , timestamp , level , xray_trace_id , and datefmt lambda_handler.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # override default values for location and timestamp format logger = Logger ( stream = stdout , location = \"[ %(funcName)s ] %(module)s \" , datefmt = \"fake-datefmt\" ) # suppress location key logger = Logger ( stream = stdout , location = None ) Alternatively, you can also change the order of the following log record keys via the log_record_order parameter: level , location , message , xray_trace_id , and timestamp lambda_handler.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger # make message as the first key logger = Logger ( stream = stdout , log_record_order = [ \"message\" ]) # Default key sorting order logger = Logger ( stream = stdout , log_record_order = [ \"level\" , \"location\" , \"message\" , \"timestamp\" ]) Some keys cannot be supressed in the Log records: sampling_rate is part of the specification and cannot be supressed; xray_trace_id is supressed automatically if X-Ray is not enabled in the Lambda function, and added automatically if it is.","title":"overriding Log records"},{"location":"core/logger/#logging-exceptions","text":"When logging exceptions, Logger will add a new key named exception , and will serialize the full traceback as a string. logging_an_exception.py 1 2 3 4 5 6 7 from aws_lambda_powertools import Logger logger = Logger () try : raise ValueError ( \"something went wrong\" ) except Exception : logger . exception ( \"Received an exception\" ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 { \"level\" : \"ERROR\" , \"location\" : \"<module>:4\" , \"message\" : \"Received an exception\" , \"timestamp\" : \"2020-08-28 18:11:38,886\" , \"service\" : \"service_undefined\" , \"sampling_rate\" : 0.0 , \"exception\" : \"Traceback (most recent call last):\\n File \\\"<input>\\\", line 2, in <module>\\nValueError: something went wrong\" }","title":"logging exceptions"},{"location":"core/logger/#testing-your-code","text":"When unit testing your code that makes use of inject_lambda_context decorator, you need to pass a dummy Lambda Context, or else Logger will fail. This is a Pytest sample that provides the minimum information necessary for Logger to succeed: fake_lambda_context_for_logger.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @pytest . fixture def lambda_context (): lambda_context = { \"function_name\" : \"test\" , \"memory_limit_in_mb\" : 128 , \"invoked_function_arn\" : \"arn:aws:lambda:eu-west-1:809313241:function:test\" , \"aws_request_id\" : \"52fdfc07-2182-154f-163f-5f0f9a621d72\" , } return namedtuple ( \"LambdaContext\" , lambda_context . keys ())( * lambda_context . values ()) def test_lambda_handler ( lambda_handler , lambda_context ): test_event = { 'test' : 'event' } lambda_handler ( test_event , lambda_context ) # this will now have a Context object populated","title":"Testing your code"},{"location":"core/logger/#pytest-live-log-feature","text":"Pytest Live Log feature duplicates emitted log messages in order to style log statements according to their levels, for this to work use POWERTOOLS_LOG_DEDUPLICATION_DISABLED env var. 1 POWERTOOLS_LOG_DEDUPLICATION_DISABLED = \"1\" pytest -o log_cli = 1 Warning This feature should be used with care, as it explicitly disables our ability to filter propagated messages to the root logger (if configured).","title":"pytest live log feature"},{"location":"core/logger/#faq","text":"How can I enable boto3 and botocore library logging? You can enable the botocore and boto3 logs by using the set_stream_logger method, this method will add a stream handler for the given name and level to the logging module. By default, this logs all boto3 messages to stdout. log_botocore_and_boto3.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from typing import Dict , List from aws_lambda_powertools.utilities.typing import LambdaContext from aws_lambda_powertools import Logger import boto3 boto3 . set_stream_logger () boto3 . set_stream_logger ( 'botocore' ) logger = Logger () client = boto3 . client ( 's3' ) def handler ( event : Dict , context : LambdaContext ) -> List : response = client . list_buckets () return response . get ( \"Buckets\" , []) What's the difference between structure_log and extra ? Keys added with structure_log will persist across multiple log messages while keys added via extra will only be available in a given log message operation. Example - Persisting payment_id not request_id lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools import Logger logger = Logger ( service = \"payment\" ) logger . structure_logs ( append = True , payment_id = \"123456789\" ) try : booking_id = book_flight () logger . info ( \"Flight booked successfully\" , extra = { \"booking_id\" : booking_id }) except BookingReservationError : ... logger . info ( \"goodbye\" ) Note that payment_id remains in both log messages while booking_id is only available in the first message. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 { \"level\" : \"INFO\" , \"location\" : \"<module>:5\" , \"message\" : \"Flight booked successfully\" , \"timestamp\" : \"2021-01-12 14:09:10,859\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" , \"booking_id\" : \"75edbad0-0857-4fc9-b547-6180e2f7959b\" }, { \"level\" : \"INFO\" , \"location\" : \"<module>:6\" , \"message\" : \"goodbye\" , \"timestamp\" : \"2021-01-12 14:09:10,860\" , \"service\" : \"payment\" , \"sampling_rate\" : 0.0 , \"payment_id\" : \"123456789\" }","title":"FAQ"},{"location":"core/metrics/","text":"Metrics creates custom metrics asynchronously by logging metrics to standard output following Amazon CloudWatch Embedded Metric Format (EMF). These metrics can be visualized through Amazon CloudWatch Console . Key features Aggregate up to 100 metrics using a single CloudWatch EMF object (large JSON blob) Validate against common metric definitions mistakes (metric unit, values, max dimensions, max metrics, etc) Metrics are created asynchronously by CloudWatch service, no custom stacks needed Context manager to create an one off metric with a different dimension Initialization Set POWERTOOLS_SERVICE_NAME and POWERTOOLS_METRICS_NAMESPACE env vars as a start - Here is an example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline We recommend you use your application or main service as a metric namespace. You can explicitly set a namespace name via namespace param or via POWERTOOLS_METRICS_NAMESPACE env var. This sets namespace key that will be used for all metrics. You can also pass a service name via service param or POWERTOOLS_SERVICE_NAME env var. This will create a dimension with the service name. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit # POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME defined metrics = Metrics () # Explicit definition Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # creates a default dimension {\"service\": \"orders\"} under the namespace \"ServerlessAirline\" You can initialize Metrics anywhere in your code as many times as you need - It'll keep track of your aggregate metrics in memory. Creating metrics You can create metrics using add_metric , and manually create dimensions for all your aggregate metrics using add_dimension . app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". CloudWatch EMF supports a max of 100 metrics. Metrics utility will flush all metrics when adding the 100th metric while subsequent metrics will be aggregated into a new EMF object, for your convenience. Creating a metric with a different dimension CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ... Adding metadata You can use add_metadata for advanced use cases, where you want to metadata as part of the serialized metrics object. This will be available in CloudWatch Logs to ease operations on high cardinal data. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" } Flushing metrics As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that right before you return your response to the caller via log_metrics . lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( service = \"ExampleService\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"service\" , value = \"booking\" ) metrics . add_metric ( name = \"BookingConfirmation\" , unit = \"Count\" , value = 1 ) ... log_metrics decorator validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Minimum of 1 dimension Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: lambda_handler.py 1 2 3 4 5 from aws_lambda_powertools.metrics import Metrics @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Info If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Warning When nesting multiple middlewares, you should use log_metrics as your last decorator wrapping all subsequent ones . lambda_handler_nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = \"Count\" , value = 1 ) ... Flushing metrics manually If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object )) Capturing cold start metric You can capture cold start metrics automatically with log_metrics via capture_cold_start_metric param. lambda_handler.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics. Testing your code Environment variables Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName) Clearing metrics Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start yield","title":"Metrics"},{"location":"core/metrics/#initialization","text":"Set POWERTOOLS_SERVICE_NAME and POWERTOOLS_METRICS_NAMESPACE env vars as a start - Here is an example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Environment : Variables : POWERTOOLS_SERVICE_NAME : payment POWERTOOLS_METRICS_NAMESPACE : ServerlessAirline We recommend you use your application or main service as a metric namespace. You can explicitly set a namespace name via namespace param or via POWERTOOLS_METRICS_NAMESPACE env var. This sets namespace key that will be used for all metrics. You can also pass a service name via service param or POWERTOOLS_SERVICE_NAME env var. This will create a dimension with the service name. app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit # POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME defined metrics = Metrics () # Explicit definition Metrics ( namespace = \"ServerlessAirline\" , service = \"orders\" ) # creates a default dimension {\"service\": \"orders\"} under the namespace \"ServerlessAirline\" You can initialize Metrics anywhere in your code as many times as you need - It'll keep track of your aggregate metrics in memory.","title":"Initialization"},{"location":"core/metrics/#creating-metrics","text":"You can create metrics using add_metric , and manually create dimensions for all your aggregate metrics using add_dimension . app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_dimension ( name = \"environment\" , value = \"prod\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) MetricUnit enum facilitate finding a supported metric unit by CloudWatch. Alternatively, you can pass the value as a string if you already know them e.g. \"Count\". CloudWatch EMF supports a max of 100 metrics. Metrics utility will flush all metrics when adding the 100th metric while subsequent metrics will be aggregated into a new EMF object, for your convenience.","title":"Creating metrics"},{"location":"core/metrics/#creating-a-metric-with-a-different-dimension","text":"CloudWatch EMF uses the same dimensions across all your metrics. Use single_metric if you have a metric that should have different dimensions. Info Generally, this would be an edge case since you pay for unique metric . Keep the following formula in mind: unique metric = (metric_name + dimension_name + dimension_value) single_metric.py 1 2 3 4 5 6 from aws_lambda_powertools import single_metric from aws_lambda_powertools.metrics import MetricUnit with single_metric ( name = \"ColdStart\" , unit = MetricUnit . Count , value = 1 , namespace = \"ExampleApplication\" ) as metric : metric . add_dimension ( name = \"function_context\" , value = \"$LATEST\" ) ...","title":"Creating a metric with a different dimension"},{"location":"core/metrics/#adding-metadata","text":"You can use add_metadata for advanced use cases, where you want to metadata as part of the serialized metrics object. This will be available in CloudWatch Logs to ease operations on high cardinal data. Info This will not be available during metrics visualization - Use dimensions for this purpose app.py 1 2 3 4 5 6 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"SuccessfulBooking\" , unit = MetricUnit . Count , value = 1 ) metrics . add_metadata ( key = \"booking_id\" , value = \"booking_uuid\" ) CloudWatch Logs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \"SuccessfulBooking\" : 1.0 , \"_aws\" : { \"Timestamp\" : 1592234975665 , \"CloudWatchMetrics\" : [ { \"Namespace\" : \"ExampleApplication\" , \"Dimensions\" : [ [ \"service\" ] ], \"Metrics\" : [ { \"Name\" : \"SuccessfulBooking\" , \"Unit\" : \"Count\" } ] } ] }, \"service\" : \"booking\" , \"booking_id\" : \"booking_uuid\" }","title":"Adding metadata"},{"location":"core/metrics/#flushing-metrics","text":"As you finish adding all your metrics, you need to serialize and flush them to standard output. You can do that right before you return your response to the caller via log_metrics . lambda_handler.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( service = \"ExampleService\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) @metrics . log_metrics def lambda_handler ( evt , ctx ): metrics . add_dimension ( name = \"service\" , value = \"booking\" ) metrics . add_metric ( name = \"BookingConfirmation\" , unit = \"Count\" , value = 1 ) ... log_metrics decorator validates , serializes , and flushes all your metrics. During metrics validation, if no metrics are provided then a warning will be logged, but no exception will be raised. If metrics are provided, and any of the following criteria are not met, SchemaValidationError exception will be raised: Minimum of 1 dimension Maximum of 9 dimensions Namespace is set, and no more than one Metric units must be supported by CloudWatch If you want to ensure that at least one metric is emitted, you can pass raise_on_empty_metrics to the log_metrics decorator: lambda_handler.py 1 2 3 4 5 from aws_lambda_powertools.metrics import Metrics @metrics . log_metrics ( raise_on_empty_metrics = True ) def lambda_handler ( evt , ctx ): ... Info If you expect your function to execute without publishing metrics every time, you can suppress the warning with warnings.filterwarnings(\"ignore\", \"No metrics to publish*\") . Warning When nesting multiple middlewares, you should use log_metrics as your last decorator wrapping all subsequent ones . lambda_handler_nested_middlewares.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) @metrics . log_metrics @tracer . capture_lambda_handler def lambda_handler ( evt , ctx ): metrics . add_metric ( name = \"BookingConfirmation\" , unit = \"Count\" , value = 1 ) ...","title":"Flushing metrics"},{"location":"core/metrics/#flushing-metrics-manually","text":"If you prefer not to use log_metrics because you might want to encapsulate additional logic when doing so, you can manually flush and clear metrics as follows: Warning Metrics, dimensions and namespace validation still applies. manual_metric_serialization.py 1 2 3 4 5 6 7 8 9 10 import json from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( namespace = \"ExampleApplication\" , service = \"booking\" ) metrics . add_metric ( name = \"ColdStart\" , unit = \"Count\" , value = 1 ) your_metrics_object = metrics . serialize_metric_set () metrics . clear_metrics () print ( json . dumps ( your_metrics_object ))","title":"Flushing metrics manually"},{"location":"core/metrics/#capturing-cold-start-metric","text":"You can capture cold start metrics automatically with log_metrics via capture_cold_start_metric param. lambda_handler.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Metrics from aws_lambda_powertools.metrics import MetricUnit metrics = Metrics ( service = \"ExampleService\" ) @metrics . log_metrics ( capture_cold_start_metric = True ) def lambda_handler ( evt , ctx ): ... If it's a cold start invocation, this feature will: Create a separate EMF blob solely containing a metric named ColdStart Add function_name and service dimensions This has the advantage of keeping cold start metric separate from your application metrics.","title":"Capturing cold start metric"},{"location":"core/metrics/#testing-your-code","text":"","title":"Testing your code"},{"location":"core/metrics/#environment-variables","text":"Use POWERTOOLS_METRICS_NAMESPACE and POWERTOOLS_SERVICE_NAME env vars when unit testing your code to ensure metric namespace and dimension objects are created, and your code doesn't fail validation. 1 POWERTOOLS_SERVICE_NAME = \"Example\" POWERTOOLS_METRICS_NAMESPACE = \"Application\" python -m pytest If you prefer setting environment variable for specific tests, and are using Pytest, you can use monkeypatch fixture: pytest_env_var.py 1 2 3 4 5 6 def test_namespace_env_var ( monkeypatch ): # Set POWERTOOLS_METRICS_NAMESPACE before initializating Metrics monkeypatch . setenv ( \"POWERTOOLS_METRICS_NAMESPACE\" , namespace ) metrics = Metrics () ... Ignore this, if you are explicitly setting namespace/default dimension via namespace and service parameters: metrics = Metrics(namespace=ApplicationName, service=ServiceName)","title":"Environment variables"},{"location":"core/metrics/#clearing-metrics","text":"Metrics keep metrics in memory across multiple instances. If you need to test this behaviour, you can use the following Pytest fixture to ensure metrics are reset incl. cold start: pytest_metrics_reset_fixture.py 1 2 3 4 5 6 7 @pytest . fixture ( scope = \"function\" , autouse = True ) def reset_metric_set (): # Clear out every metric data prior to every test metrics = Metrics () metrics . clear_metrics () metrics_global . is_cold_start = True # ensure each test has cold start yield","title":"Clearing metrics"},{"location":"core/tracer/","text":"Tracer is an opinionated thin wrapper for AWS X-Ray Python SDK . Key features Capture cold start as annotation, and responses as well as full exceptions as metadata Run functions locally with SAM CLI without code change to disable tracing Explicitly disable tracing via env var POWERTOOLS_TRACE_DISABLED=\"true\" Support tracing async methods, generators, and context managers Auto patch supported modules, or a tuple of explicit modules supported by AWS X-Ray Initialization Your AWS Lambda function must have permission to send traces to AWS X-Ray - Here is an example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example You can either explicitly pass using service param or via POWERTOOLS_SERVICE_NAME environment variable. The service name is utilized for exceptions, metadata, and namespace data. 1 2 3 4 5 6 from aws_lambda_powertools import Tracer # POWERTOOLS_SERVICE_NAME defined tracer = Tracer () # Explicit definition tracer = Tracer ( service = \"booking\" ) Lambda handler You can trace your Lambda function handler via capture_lambda_handler . When using this decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... disabling response auto-capture New in 1.9.0 Warning Returning sensitive information from your Lambda handler or functions, where Tracer is used? You can disable Tracer from capturing their responses as tracing metadata with capture_response=False parameter in both capture_lambda_handler and capture_method decorators. 1 2 3 4 5 # Disables Tracer from capturing response and adding as metadata # Useful when dealing with sensitive data @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): return \"sensitive_information\" disabling exception auto-capture New in 1.10.0 Warning Can exceptions contain sensitive information from your Lambda handler or functions, where Tracer is used? You can disable Tracer from capturing their exceptions as tracing metadata with capture_error=False parameter in both capture_lambda_handler and capture_method decorators. 1 2 3 4 5 # Disables Tracer from capturing exception and adding as metadata # Useful when dealing with sensitive data @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" ) Annotations & Metadata Annotations are key-values indexed by AWS X-Ray on a per trace basis. You can use them to filter traces as well as to create Trace Groups . You can add annotations using put_annotation method from Tracer. Metadata are non-indexed values that can add additional context for an operation. You can add metadata using put_metadata method from Tracer. Annotations 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret ) Synchronous functions You can trace a synchronous function using the capture_method . Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret @tracer . capture_method ( capture_response = False ) def sensitive_information_to_be_processed (): return \"sensitive_information\" # If we capture response, the s3_object[\"Body\"].read() method will be called by x-ray-sdk when # trying to serialize the object. This will cause it to return empty next time it is called. @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object Asynchronous and generator functions Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ()) Patching modules Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched ) Tracing aiohttp requests Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. 1 2 3 4 5 6 7 8 9 10 11 12 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp Escape hatch mechanism You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers . Concurrent asynchronous functions Info As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ... Reusing Tracer across your code Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. 1 2 3 4 5 6 7 8 9 # handler.py from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... 1 2 3 from aws_lambda_powertools import Tracer # new instance using existing configuration tracer = Tracer ( service = \"payment\" ) Testing your code You can safely disable Tracer when unit testing your code using POWERTOOLS_TRACE_DISABLED environment variable. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest Tips Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tracer"},{"location":"core/tracer/#initialization","text":"Your AWS Lambda function must have permission to send traces to AWS X-Ray - Here is an example using AWS Serverless Application Model (SAM) template.yml 1 2 3 4 5 6 7 8 9 10 Resources : HelloWorldFunction : Type : AWS::Serverless::Function Properties : ... Runtime : python3.8 Tracing : Active Environment : Variables : POWERTOOLS_SERVICE_NAME : example You can either explicitly pass using service param or via POWERTOOLS_SERVICE_NAME environment variable. The service name is utilized for exceptions, metadata, and namespace data. 1 2 3 4 5 6 from aws_lambda_powertools import Tracer # POWERTOOLS_SERVICE_NAME defined tracer = Tracer () # Explicit definition tracer = Tracer ( service = \"booking\" )","title":"Initialization"},{"location":"core/tracer/#lambda-handler","text":"You can trace your Lambda function handler via capture_lambda_handler . When using this decorator, Tracer performs these additional tasks to ease operations: Creates a ColdStart annotation to easily filter traces that have had an initialization overhead Captures any response, or full exceptions generated by the handler, and include as tracing metadata 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ...","title":"Lambda handler"},{"location":"core/tracer/#disabling-response-auto-capture","text":"New in 1.9.0 Warning Returning sensitive information from your Lambda handler or functions, where Tracer is used? You can disable Tracer from capturing their responses as tracing metadata with capture_response=False parameter in both capture_lambda_handler and capture_method decorators. 1 2 3 4 5 # Disables Tracer from capturing response and adding as metadata # Useful when dealing with sensitive data @tracer . capture_lambda_handler ( capture_response = False ) def handler ( event , context ): return \"sensitive_information\"","title":"disabling response auto-capture"},{"location":"core/tracer/#disabling-exception-auto-capture","text":"New in 1.10.0 Warning Can exceptions contain sensitive information from your Lambda handler or functions, where Tracer is used? You can disable Tracer from capturing their exceptions as tracing metadata with capture_error=False parameter in both capture_lambda_handler and capture_method decorators. 1 2 3 4 5 # Disables Tracer from capturing exception and adding as metadata # Useful when dealing with sensitive data @tracer . capture_lambda_handler ( capture_error = False ) def handler ( event , context ): raise ValueError ( \"some sensitive info in the stack trace...\" )","title":"disabling exception auto-capture"},{"location":"core/tracer/#annotations-metadata","text":"Annotations are key-values indexed by AWS X-Ray on a per trace basis. You can use them to filter traces as well as to create Trace Groups . You can add annotations using put_annotation method from Tracer. Metadata are non-indexed values that can add additional context for an operation. You can add metadata using put_metadata method from Tracer. Annotations 1 2 3 4 5 6 7 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... tracer . put_annotation ( key = \"PaymentStatus\" , value = \"SUCCESS\" ) Metadata 1 2 3 4 5 6 7 8 from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_lambda_handler def handler ( event , context ): ... ret = some_logic () tracer . put_metadata ( key = \"payment_response\" , value = ret )","title":"Annotations &amp; Metadata"},{"location":"core/tracer/#synchronous-functions","text":"You can trace a synchronous function using the capture_method . Warning When capture_response is enabled, the function response will be read and serialized as json. The serialization is performed by the aws-xray-sdk which uses the jsonpickle module. This can cause unintended consequences if there are side effects to recursively reading the returned value, for example if the decorated function response contains a file-like object or a StreamingBody for S3 objects. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @tracer . capture_method def collect_payment ( charge_id ): ret = requests . post ( PAYMENT_ENDPOINT ) # logic tracer . put_annotation ( \"PAYMENT_STATUS\" , \"SUCCESS\" ) # custom annotation return ret @tracer . capture_method ( capture_response = False ) def sensitive_information_to_be_processed (): return \"sensitive_information\" # If we capture response, the s3_object[\"Body\"].read() method will be called by x-ray-sdk when # trying to serialize the object. This will cause it to return empty next time it is called. @tracer . capture_method ( capture_response = False ) def get_s3_object ( bucket_name , object_key ): s3 = boto3 . client ( \"s3\" ) s3_object = get_object ( Bucket = bucket_name , Key = object_key ) return s3_object","title":"Synchronous functions"},{"location":"core/tracer/#asynchronous-and-generator-functions","text":"Warning We do not support async Lambda handler - Lambda handler itself must be synchronous You can trace asynchronous functions and generator functions (including context managers) using capture_method . Async 1 2 3 4 5 6 7 8 import asyncio import contextlib from aws_lambda_powertools import Tracer tracer = Tracer () @tracer . capture_method async def collect_payment (): ... Context manager 1 2 3 4 5 @contextlib . contextmanager @tracer . capture_method def collect_payment_ctxman (): yield result ... Generators 1 2 3 4 @tracer . capture_method def collect_payment_gen (): yield result ... The decorator will detect whether your function is asynchronous, a generator, or a context manager and adapt its behaviour accordingly. 1 2 3 4 5 6 7 8 @tracer . capture_lambda_handler def handler ( evt , ctx ): asyncio . run ( collect_payment ()) with collect_payment_ctxman as result : do_something_with ( result ) another_result = list ( collect_payment_gen ())","title":"Asynchronous and generator functions"},{"location":"core/tracer/#patching-modules","text":"Tracer automatically patches all supported libraries by X-Ray during initialization, by default. Underneath, AWS X-Ray SDK checks whether a supported library has been imported before patching. If you're looking to shave a few microseconds, or milliseconds depending on your function memory configuration, you can patch specific modules using patch_modules param: 1 2 3 4 5 6 7 import boto3 import requests from aws_lambda_powertools import Tracer modules_to_be_patched = [ \"boto3\" , \"requests\" ] tracer = Tracer ( patch_modules = modules_to_be_patched )","title":"Patching modules"},{"location":"core/tracer/#tracing-aiohttp-requests","text":"Info This snippet assumes you have aiohttp as a dependency You can use aiohttp_trace_config function to create a valid aiohttp trace_config object . This is necessary since X-Ray utilizes aiohttp trace hooks to capture requests end-to-end. 1 2 3 4 5 6 7 8 9 10 11 12 import asyncio import aiohttp from aws_lambda_powertools import Tracer from aws_lambda_powertools.tracing import aiohttp_trace_config tracer = Tracer () async def aiohttp_task (): async with aiohttp . ClientSession ( trace_configs = [ aiohttp_trace_config ()]) as session : async with session . get ( \"https://httpbin.org/json\" ) as resp : resp = await resp . json () return resp","title":"Tracing aiohttp requests"},{"location":"core/tracer/#escape-hatch-mechanism","text":"You can use tracer.provider attribute to access all methods provided by AWS X-Ray xray_recorder object. This is useful when you need a feature available in X-Ray that is not available in the Tracer utility, for example thread-safe , or context managers .","title":"Escape hatch mechanism"},{"location":"core/tracer/#concurrent-asynchronous-functions","text":"Info As of now, X-Ray SDK will raise an exception when async functions are run and traced concurrently A safe workaround mechanism is to use in_subsegment_async available via Tracer escape hatch ( tracer.provider ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import asyncio from aws_lambda_powertools import Tracer tracer = Tracer () async def another_async_task (): async with tracer . provider . in_subsegment_async ( \"## another_async_task\" ) as subsegment : subsegment . put_annotation ( key = \"key\" , value = \"value\" ) subsegment . put_metadata ( key = \"key\" , value = \"value\" , namespace = \"namespace\" ) ... async def another_async_task_2 (): ... @tracer . capture_method async def collect_payment ( charge_id ): asyncio . gather ( another_async_task (), another_async_task_2 ()) ...","title":"Concurrent asynchronous functions"},{"location":"core/tracer/#reusing-tracer-across-your-code","text":"Tracer keeps a copy of its configuration after the first initialization. This is useful for scenarios where you want to use Tracer in more than one location across your code base. Warning When reusing Tracer in Lambda Layers, or in multiple modules, do not set auto_patch=False , because import order matters. This can result in the first Tracer config being inherited by new instances, and their modules not being patched. 1 2 3 4 5 6 7 8 9 # handler.py from aws_lambda_powertools import Tracer tracer = Tracer ( service = \"payment\" ) @tracer . capture_lambda_handler def handler ( event , context ): charge_id = event . get ( 'charge_id' ) payment = collect_payment ( charge_id ) ... 1 2 3 from aws_lambda_powertools import Tracer # new instance using existing configuration tracer = Tracer ( service = \"payment\" )","title":"Reusing Tracer across your code"},{"location":"core/tracer/#testing-your-code","text":"You can safely disable Tracer when unit testing your code using POWERTOOLS_TRACE_DISABLED environment variable. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Testing your code"},{"location":"core/tracer/#tips","text":"Use annotations on key operations to slice and dice traces, create unique views, and create metrics from it via Trace Groups Use a namespace when adding metadata to group data more easily Annotations and metadata are added to the current subsegment opened. If you want them in a specific subsegment, use a context manager via the escape hatch mechanism","title":"Tips"},{"location":"utilities/batch/","text":"The SQS batch processing utility provides a way to handle partial failures when processing batches of messages from SQS. Key Features Prevent successfully processed messages being returned to SQS Simple interface for individually processing messages from a batch Build your own batch processor using the base classes Background When using SQS as a Lambda event source mapping, Lambda functions are triggered with a batch of messages from SQS. If your function fails to process any message from the batch, the entire batch returns to your SQS queue, and your Lambda function is triggered with the same batch one more time. With this utility, messages within a batch are handled individually - only messages that were not successfully processed are returned to the queue. Warning While this utility lowers the chance of processing messages more than once, it is not guaranteed. We recommend implementing processing logic in an idempotent manner wherever possible. More details on how Lambda works with SQS can be found in the AWS documentation IAM Permissions This utility requires additional permissions to work as expected. Lambda functions using this utility require the sqs:DeleteMessageBatch permission. Processing messages from SQS You can use either sqs_batch_processor decorator, or PartialSQSProcessor as a context manager. They have nearly the same behaviour when it comes to processing messages from the batch: Entire batch has been successfully processed , where your Lambda handler returned successfully, we will let SQS delete the batch to optimize your cost Entire Batch has been partially processed successfully , where exceptions were raised within your record handler , we will: 1) Delete successfully processed messages from the queue by directly calling sqs:DeleteMessageBatch 2) Raise SQSBatchProcessingError to ensure failed messages return to your SQS queue The only difference is that PartialSQSProcessor will give you access to processed messages if you need. Record Handler Both decorator and context managers require an explicit function to process the batch of messages - namely record_handler parameter. This function is responsible for processing each individual message from the batch, and to raise an exception if unable to process any of the messages sent. Any non-exception/successful return from your record handler function will instruct both decorator and context manager to queue up each individual message for deletion. sqs_batch_processor decorator When using this decorator, you need provide a function via record_handler param that will process individual messages from the batch - It should raise an exception if it is unable to process the record. All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: Any successfully processed messages , we will delete them from the queue via sqs:DeleteMessageBatch Any unprocessed messages detected , we will raise SQSBatchProcessingError to ensure failed messages return to your SQS queue Warning You will not have accessed to the processed messages within the Lambda Handler - all processing logic will and should be performed by the record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } PartialSQSProcessor context manager If you require access to the result of processed messages, you can use this context manager. The result from calling process() on the context manager will be a list of all the return values from your record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result Passing custom boto3 config If you need to pass custom configuration such as region to the SDK, you can pass your own botocore config object to the sqs_batch_processor decorator: Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result Suppressing exceptions If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 ... @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 ... processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process () Create your own partial processor You can create your own partial batch processor by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() - Handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() - Called once as part of the processor initialization clean() - Teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. Example: custom_processor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results # E.g.: self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table # E.g.: with ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument # E.g.: try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 }","title":"SQS Batch Processing"},{"location":"utilities/batch/#processing-messages-from-sqs","text":"You can use either sqs_batch_processor decorator, or PartialSQSProcessor as a context manager. They have nearly the same behaviour when it comes to processing messages from the batch: Entire batch has been successfully processed , where your Lambda handler returned successfully, we will let SQS delete the batch to optimize your cost Entire Batch has been partially processed successfully , where exceptions were raised within your record handler , we will: 1) Delete successfully processed messages from the queue by directly calling sqs:DeleteMessageBatch 2) Raise SQSBatchProcessingError to ensure failed messages return to your SQS queue The only difference is that PartialSQSProcessor will give you access to processed messages if you need.","title":"Processing messages from SQS"},{"location":"utilities/batch/#record-handler","text":"Both decorator and context managers require an explicit function to process the batch of messages - namely record_handler parameter. This function is responsible for processing each individual message from the batch, and to raise an exception if unable to process any of the messages sent. Any non-exception/successful return from your record handler function will instruct both decorator and context manager to queue up each individual message for deletion.","title":"Record Handler"},{"location":"utilities/batch/#sqs_batch_processor-decorator","text":"When using this decorator, you need provide a function via record_handler param that will process individual messages from the batch - It should raise an exception if it is unable to process the record. All records in the batch will be passed to this handler for processing, even if exceptions are thrown - Here's the behaviour after completing the batch: Any successfully processed messages , we will delete them from the queue via sqs:DeleteMessageBatch Any unprocessed messages detected , we will raise SQSBatchProcessingError to ensure failed messages return to your SQS queue Warning You will not have accessed to the processed messages within the Lambda Handler - all processing logic will and should be performed by the record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.batch import sqs_batch_processor def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 }","title":"sqs_batch_processor decorator"},{"location":"utilities/batch/#partialsqsprocessor-context-manager","text":"If you require access to the result of processed messages, you can use this context manager. The result from calling process() on the context manager will be a list of all the return values from your record_handler function. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor () with processor ( records , record_handler ) as proc : result = proc . process () # Returns a list of all results from record_handler return result","title":"PartialSQSProcessor context manager"},{"location":"utilities/batch/#passing-custom-boto3-config","text":"If you need to pass custom configuration such as region to the SDK, you can pass your own botocore config object to the sqs_batch_processor decorator: Decorator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.batch import sqs_batch_processor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value @sqs_batch_processor ( record_handler = record_handler , config = config ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.batch import PartialSQSProcessor from botocore.config import Config config = Config ( region_name = \"us-east-1\" ) def record_handler ( record ): # This will be called for each individual message from a batch # It should raise an exception if the message was not processed successfully return_value = do_something_with ( record [ \"body\" ]) return return_value def lambda_handler ( event , context ): records = event [ \"Records\" ] processor = PartialSQSProcessor ( config = config ) with processor ( records , record_handler ): result = processor . process () return result","title":"Passing custom boto3 config"},{"location":"utilities/batch/#suppressing-exceptions","text":"If you want to disable the default behavior where SQSBatchProcessingError is raised if there are any errors, you can pass the suppress_exception boolean argument. Decorator 1 2 3 4 ... @sqs_batch_processor ( record_handler = record_handler , config = config , suppress_exception = True ) def lambda_handler ( event , context ): return { \"statusCode\" : 200 } Context manager 1 2 3 4 5 ... processor = PartialSQSProcessor ( config = config , suppress_exception = True ) with processor ( records , record_handler ): result = processor . process ()","title":"Suppressing exceptions"},{"location":"utilities/batch/#create-your-own-partial-processor","text":"You can create your own partial batch processor by inheriting the BasePartialProcessor class, and implementing _prepare() , _clean() and _process_record() . _process_record() - Handles all processing logic for each individual message of a batch, including calling the record_handler (self.handler) _prepare() - Called once as part of the processor initialization clean() - Teardown logic called once after _process_record completes You can then use this class as a context manager, or pass it to batch_processor to use as a decorator on your Lambda handler function. Example: custom_processor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 from random import randint from aws_lambda_powertools.utilities.batch import BasePartialProcessor , batch_processor import boto3 import os table_name = os . getenv ( \"TABLE_NAME\" , \"table_not_found\" ) class MyPartialProcessor ( BasePartialProcessor ): \"\"\" Process a record and stores successful results at a Amazon DynamoDB Table Parameters ---------- table_name: str DynamoDB table name to write results to \"\"\" def __init__ ( self , table_name : str ): self . table_name = table_name super () . __init__ () def _prepare ( self ): # It's called once, *before* processing # Creates table resource and clean previous results # E.g.: self . ddb_table = boto3 . resource ( \"dynamodb\" ) . Table ( self . table_name ) self . success_messages . clear () def _clean ( self ): # It's called once, *after* closing processing all records (closing the context manager) # Here we're sending, at once, all successful messages to a ddb table # E.g.: with ddb_table . batch_writer () as batch : for result in self . success_messages : batch . put_item ( Item = result ) def _process_record ( self , record ): # It handles how your record is processed # Here we're keeping the status of each run # where self.handler is the record_handler function passed as an argument # E.g.: try : result = self . handler ( record ) # record_handler passed to decorator/context manager return self . success_handler ( record , result ) except Exception as exc : return self . failure_handler ( record , exc ) def success_handler ( self , record ): entry = ( \"success\" , result , record ) message = { \"age\" : result } self . success_messages . append ( message ) return entry def record_handler ( record ): return randint ( 0 , 100 ) @batch_processor ( record_handler = record_handler , processor = MyPartialProcessor ( table_name )) def lambda_handler ( event , context ): return { \"statusCode\" : 200 }","title":"Create your own partial processor"},{"location":"utilities/data_classes/","text":"The event source data classes utility provides classes describing the schema of common Lambda events triggers. Key Features Type hinting and code completion for common event types Helper functions for decoding/deserializing nested fields Docstrings for fields contained in event schemas Background When authoring Lambda functions, you often need to understand the schema of the event dictionary which is passed to the handler. There are several common event types which follow a specific schema, depending on the service triggering the Lambda function. Utilizing the data classes The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class. Supported event sources Event Source Data_class API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy event v2 APIGatewayProxyEventV2 CloudWatch Logs CloudWatchLogsEvent Cognito User Pool Multiple available under cognito_user_pool_event DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent S3 S3Event SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings). API Gateway Proxy Typically used for API Gateway REST API or HTTP API using v1 proxy event. lambda_app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) request_context = event . request_context identity = request_context . identity if 'helloworld' in event . path && event . http_method == 'GET' : user = identity . user do_something_with ( event . body , user ) API Gateway Proxy v2 lambda_app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEventV2 def lambda_handler ( event , context ): event : APIGatewayProxyEventV2 = APIGatewayProxyEventV2 ( event ) request_context = event . request_context query_string_parameters = event . query_string_parameters if 'helloworld' in event . raw_path && request_context . http . method == 'POST' : do_something_with ( event . body , query_string_parameters ) CloudWatch Logs CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. lambda_app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import CloudWatchLogsEvent def lambda_handler ( event , context ): event : CloudWatchLogsEvent = CloudWatchLogsEvent ( event ) decompressed_log = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message ) Cognito User Pool Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent lambda_app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = user_attributes = event . request . user_attributes do_something_with ( user_attributes ) DynamoDB Streams The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). lambda_app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import DynamoDBStreamEvent , DynamoDBRecordEventName def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image ) EventBridge lambda_app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import EventBridgeEvent def lambda_handler ( event , context ): event : EventBridgeEvent = EventBridgeEvent ( event ) do_something_with ( event . detail ) Kinesis streams Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.data_classes import KinesisStreamEvent def lambda_handler ( event , context ): event : KinesisStreamEvent = KinesisStreamEvent ( event ) # if data was delivered as json data = event . data_as_text () # if data was delivered as text data = event . data_as_json () do_something_with ( data ) S3 lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import S3Event def lambda_handler ( event , context ): event : S3Event = S3Event ( event ) bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = record . s3 . get_object . key do_something_with ( f ' { bucket_name } / { object_key } ' ) SES lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SESEvent def lambda_handler ( event , context ): event : SESEvent = SESEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject ) SNS lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SNSEvent def lambda_handler ( event , context ): event : SNSEvent = SNSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message ) SQS lambda_app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes import SQSEvent def lambda_handler ( event , context ): event : SQSEvent = SQSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"Event Source Data Classes"},{"location":"utilities/data_classes/#utilizing-the-data-classes","text":"The classes are initialized by passing in the Lambda event object into the constructor of the appropriate data class. For example, if your Lambda function is being triggered by an API Gateway proxy integration, you can use the APIGatewayProxyEvent class.","title":"Utilizing the data classes"},{"location":"utilities/data_classes/#supported-event-sources","text":"Event Source Data_class API Gateway Proxy APIGatewayProxyEvent API Gateway Proxy event v2 APIGatewayProxyEventV2 CloudWatch Logs CloudWatchLogsEvent Cognito User Pool Multiple available under cognito_user_pool_event DynamoDB streams DynamoDBStreamEvent , DynamoDBRecordEventName EventBridge EventBridgeEvent Kinesis Data Stream KinesisStreamEvent S3 S3Event SES SESEvent SNS SNSEvent SQS SQSEvent Info The examples provided below are far from exhaustive - the data classes themselves are designed to provide a form of documentation inherently (via autocompletion, types and docstrings).","title":"Supported event sources"},{"location":"utilities/data_classes/#api-gateway-proxy","text":"Typically used for API Gateway REST API or HTTP API using v1 proxy event. lambda_app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEvent def lambda_handler ( event , context ): event : APIGatewayProxyEvent = APIGatewayProxyEvent ( event ) request_context = event . request_context identity = request_context . identity if 'helloworld' in event . path && event . http_method == 'GET' : user = identity . user do_something_with ( event . body , user )","title":"API Gateway Proxy"},{"location":"utilities/data_classes/#api-gateway-proxy-v2","text":"lambda_app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import APIGatewayProxyEventV2 def lambda_handler ( event , context ): event : APIGatewayProxyEventV2 = APIGatewayProxyEventV2 ( event ) request_context = event . request_context query_string_parameters = event . query_string_parameters if 'helloworld' in event . raw_path && request_context . http . method == 'POST' : do_something_with ( event . body , query_string_parameters )","title":"API Gateway Proxy v2"},{"location":"utilities/data_classes/#cloudwatch-logs","text":"CloudWatch Logs events by default are compressed and base64 encoded. You can use the helper function provided to decode, decompress and parse json data from the event. lambda_app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities.data_classes import CloudWatchLogsEvent def lambda_handler ( event , context ): event : CloudWatchLogsEvent = CloudWatchLogsEvent ( event ) decompressed_log = event . parse_logs_data log_events = decompressed_log . log_events for event in log_events : do_something_with ( event . timestamp , event . message )","title":"CloudWatch Logs"},{"location":"utilities/data_classes/#cognito-user-pool","text":"Cognito User Pools have several different Lambda trigger sources , all of which map to a different data class, which can be imported from aws_lambda_powertools.data_classes.cognito_user_pool_event : Trigger/Event Source Data Class Custom message event data_classes.cognito_user_pool_event.CustomMessageTriggerEvent Post authentication data_classes.cognito_user_pool_event.PostAuthenticationTriggerEvent Post confirmation data_classes.cognito_user_pool_event.PostConfirmationTriggerEvent Pre authentication data_classes.cognito_user_pool_event.PreAuthenticationTriggerEvent Pre sign-up data_classes.cognito_user_pool_event.PreSignUpTriggerEvent Pre token generation data_classes.cognito_user_pool_event.PreTokenGenerationTriggerEvent User migration data_classes.cognito_user_pool_event.UserMigrationTriggerEvent Define Auth Challenge data_classes.cognito_user_pool_event.DefineAuthChallengeTriggerEvent Create Auth Challenge data_classes.cognito_user_pool_event.CreateAuthChallengeTriggerEvent Verify Auth Challenge data_classes.cognito_user_pool_event.VerifyAuthChallengeResponseTriggerEvent lambda_app.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities.data_classes.cognito_user_pool_event import PostConfirmationTriggerEvent def lambda_handler ( event , context ): event : PostConfirmationTriggerEvent = PostConfirmationTriggerEvent ( event ) user_attributes = user_attributes = event . request . user_attributes do_something_with ( user_attributes )","title":"Cognito User Pool"},{"location":"utilities/data_classes/#dynamodb-streams","text":"The DynamoDB data class utility provides the base class for DynamoDBStreamEvent , a typed class for attributes values ( AttributeValue ), as well as enums for stream view type ( StreamViewType ) and event type ( DynamoDBRecordEventName ). lambda_app.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities.data_classes import DynamoDBStreamEvent , DynamoDBRecordEventName def lambda_handler ( event , context ): event : DynamoDBStreamEvent = DynamoDBStreamEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : if record . event_name == DynamoDBRecordEventName . MODIFY : do_something_with ( record . dynamodb . new_image ) do_something_with ( record . dynamodb . old_image )","title":"DynamoDB Streams"},{"location":"utilities/data_classes/#eventbridge","text":"lambda_app.py 1 2 3 4 5 from aws_lambda_powertools.utilities.data_classes import EventBridgeEvent def lambda_handler ( event , context ): event : EventBridgeEvent = EventBridgeEvent ( event ) do_something_with ( event . detail )","title":"EventBridge"},{"location":"utilities/data_classes/#kinesis-streams","text":"Kinesis events by default contain base64 encoded data. You can use the helper function to access the data either as json or plain text, depending on the original payload. lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.data_classes import KinesisStreamEvent def lambda_handler ( event , context ): event : KinesisStreamEvent = KinesisStreamEvent ( event ) # if data was delivered as json data = event . data_as_text () # if data was delivered as text data = event . data_as_json () do_something_with ( data )","title":"Kinesis streams"},{"location":"utilities/data_classes/#s3","text":"lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import S3Event def lambda_handler ( event , context ): event : S3Event = S3Event ( event ) bucket_name = event . bucket_name # Multiple records can be delivered in a single event for record in event . records : object_key = record . s3 . get_object . key do_something_with ( f ' { bucket_name } / { object_key } ' )","title":"S3"},{"location":"utilities/data_classes/#ses","text":"lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SESEvent def lambda_handler ( event , context ): event : SESEvent = SESEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : mail = record . ses . mail common_headers = mail . common_headers do_something_with ( common_headers . to , common_headers . subject )","title":"SES"},{"location":"utilities/data_classes/#sns","text":"lambda_app.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.data_classes import SNSEvent def lambda_handler ( event , context ): event : SNSEvent = SNSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : message = record . sns . message subject = record . sns . subject do_something_with ( subject , message )","title":"SNS"},{"location":"utilities/data_classes/#sqs","text":"lambda_app.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.data_classes import SQSEvent def lambda_handler ( event , context ): event : SQSEvent = SQSEvent ( event ) # Multiple records can be delivered in a single event for record in event . records : do_something_with ( record . body )","title":"SQS"},{"location":"utilities/middleware_factory/","text":"Middleware factory provides a decorator factory to create your own middleware to run logic before, and after each Lambda invocation synchronously. Key features Run logic before, after, and handle exceptions Trace each middleware when requested Middleware with no params You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ... Middleware with params You can also have your own keyword arguments after the mandatory arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : field = event . get ( field , \"\" ) if field in event : event [ field ] = obfuscate ( field ) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ... Tracing middleware execution If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. trace_middleware_execution.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): tracer = Tracer () # Takes a copy of an existing tracer instance tracer . add_annotation ... tracer . add_metadata ... return handler ( event , context ) Tips Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported Testing your code When unit testing middlewares with trace_execution option enabled, use POWERTOOLS_TRACE_DISABLED env var to safely disable Tracer. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Middleware factory"},{"location":"utilities/middleware_factory/#middleware-with-no-params","text":"You can create your own middleware using lambda_handler_decorator . The decorator factory expects 3 arguments in your function signature: handler - Lambda function handler event - Lambda function invocation event context - Lambda function context object app.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator def middleware_before_after ( handler , event , context ): # logic_before_handler_execution() response = handler ( event , context ) # logic_after_handler_execution() return response @middleware_before_after def lambda_handler ( event , context ): ...","title":"Middleware with no params"},{"location":"utilities/middleware_factory/#middleware-with-params","text":"You can also have your own keyword arguments after the mandatory arguments. app.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 @lambda_handler_decorator def obfuscate_sensitive_data ( handler , event , context , fields : List = None ): # Obfuscate email before calling Lambda handler if fields : for field in fields : field = event . get ( field , \"\" ) if field in event : event [ field ] = obfuscate ( field ) return handler ( event , context ) @obfuscate_sensitive_data ( fields = [ \"email\" ]) def lambda_handler ( event , context ): ...","title":"Middleware with params"},{"location":"utilities/middleware_factory/#tracing-middleware-execution","text":"If you are making use of Tracer , you can trace the execution of your middleware to ease operations. This makes use of an existing Tracer instance that you may have initialized anywhere in your code. trace_middleware_execution.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator @lambda_handler_decorator ( trace_execution = True ) def my_middleware ( handler , event , context ): return handler ( event , context ) @my_middleware def lambda_handler ( event , context ): ... When executed, your middleware name will appear in AWS X-Ray Trace details as ## middleware_name . For advanced use cases, you can instantiate Tracer inside your middleware, and add annotations as well as metadata for additional operational insights. app.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.middleware_factory import lambda_handler_decorator from aws_lambda_powertools import Tracer @lambda_handler_decorator ( trace_execution = True ) def middleware_name ( handler , event , context ): tracer = Tracer () # Takes a copy of an existing tracer instance tracer . add_annotation ... tracer . add_metadata ... return handler ( event , context )","title":"Tracing middleware execution"},{"location":"utilities/middleware_factory/#tips","text":"Use trace_execution to quickly understand the performance impact of your middlewares, and reduce or merge tasks when necessary When nesting multiple middlewares, always return the handler with event and context, or response Keep in mind Python decorators execution order . Lambda handler is actually called once (top-down) Async middlewares are not supported","title":"Tips"},{"location":"utilities/middleware_factory/#testing-your-code","text":"When unit testing middlewares with trace_execution option enabled, use POWERTOOLS_TRACE_DISABLED env var to safely disable Tracer. 1 POWERTOOLS_TRACE_DISABLED = 1 python -m pytest","title":"Testing your code"},{"location":"utilities/parameters/","text":"The parameters utility provides a way to retrieve parameter values from AWS Systems Manager Parameter Store , AWS Secrets Manager or Amazon DynamoDB . It also provides a base class to create your parameter provider implementation. Key features Retrieve one or multiple parameters from the underlying provider Cache parameter values for a given amount of time (defaults to 5 seconds) Transform parameter values from JSON or base 64 encoded strings IAM Permissions This utility requires additional permissions to work as expected. See the table below: Provider Function/Method IAM Permission SSM Parameter Store get_parameter , SSMProvider.get ssm:GetParameter SSM Parameter Store get_parameters , SSMProvider.get_multiple ssm:GetParametersByPath Secrets Manager get_secret , SecretsManager.get secretsmanager:GetSecretValue DynamoDB DynamoDBProvider.get dynamodb:GetItem DynamoDB DynamoDBProvider.get_multiple dynamodb:Query App Config AppConfigProvider.get_app_config , get_app_config appconfig:GetConfiguration SSM Parameter Store You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) SSMProvider class Alternatively, you can use the SSMProvider class, which gives more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example: ssm_parameter_store.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False ) Secrets Manager For secrets stored in Secrets Manager, use get_secret . secrets_manager.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" ) SecretsProvider class Alternatively, you can use the SecretsProvider class, which give more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. secrets_manager.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" ) DynamoDB To use the DynamoDB provider, you need to import and instantiate the DynamoDBProvider class. The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure When using the default options, if you want to retrieve only single parameters, your table should be structured as such, assuming a parameter named my-parameter with a value of my-value . The id attribute should be the partition key for that table. id value my-parameter my-value With this table, when you do a dynamodb_provider.get(\"my-param\") call, this will return my-value . dynamodb.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) Retrieve multiple values If you want to be able to retrieve multiple parameters at once sharing the same id , your table needs to contain a sort key name sk . For example, if you want to retrieve multiple parameters having my-hash-key as ID: id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, when you do a dynamodb_provider.get_multiple(\"my-hash-key\") call, you will receive the following dict as a response: 1 2 3 4 5 { \"param-a\": \"my-value-a\", \"param-b\": \"my-value-b\", \"param-c\": \"my-value-c\" } Example: dynamodb_multiple.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. values = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The Amazon DynamoDB provider supports four additional arguments at initialization: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. dynamodb.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" ) App Config New in 1.10.0 For configurations stored in App Config, use get_app_config . The following will retrieve the latest version and store it in the cache. appconfig.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" ) AppConfigProvider class Alternatively, you can use the AppConfigProvider class, which give more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. appconfig.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" ) Create your own provider You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: custom_provider.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters Transform values For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization - The transform argument is available across all providers, including the high level functions. transform.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" ) You can also use the transform argument with high-level functions: transform.py 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" ) Partial transform failures with get_multiple() If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters ( /param/a , /param/b and /param/c ) but /param/c is malformed: partial_failures.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True ) Additional SDK arguments You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. ssm_parameter_store.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration","title":"Parameters"},{"location":"utilities/parameters/#ssm-parameter-store","text":"You can retrieve a single parameter using get_parameter high-level function. For multiple parameters, you can use get_parameters and pass a path to retrieve them recursively. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single parameter value = parameters . get_parameter ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix recursively # This returns a dict with the parameter name as key values = parameters . get_parameters ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" )","title":"SSM Parameter Store"},{"location":"utilities/parameters/#ssmprovider-class","text":"Alternatively, you can use the SSMProvider class, which gives more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. ssm_parameter_store.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) ssm_provider = parameters . SSMProvider ( config = config ) def handler ( event , context ): # Retrieve a single parameter value = ssm_provider . get ( \"/my/parameter\" ) # Retrieve multiple parameters from a path prefix values = ssm_provider . get_multiple ( \"/my/path/prefix\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The AWS Systems Manager Parameter Store provider supports two additional arguments for the get() and get_multiple() methods: Parameter Default Description decrypt False Will automatically decrypt the parameter. recursive True For get_multiple() only, will fetch all parameter values recursively based on a path prefix. Example: ssm_parameter_store.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): decrypted_value = ssm_provider . get ( \"/my/encrypted/parameter\" , decrypt = True ) no_recursive_values = ssm_provider . get_multiple ( \"/my/path/prefix\" , recursive = False )","title":"SSMProvider class"},{"location":"utilities/parameters/#secrets-manager","text":"For secrets stored in Secrets Manager, use get_secret . secrets_manager.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single secret value = parameters . get_secret ( \"my-secret\" )","title":"Secrets Manager"},{"location":"utilities/parameters/#secretsprovider-class","text":"Alternatively, you can use the SecretsProvider class, which give more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. secrets_manager.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) secrets_provider = parameters . SecretsProvider ( config = config ) def handler ( event , context ): # Retrieve a single secret value = secrets_provider . get ( \"my-secret\" )","title":"SecretsProvider class"},{"location":"utilities/parameters/#dynamodb","text":"To use the DynamoDB provider, you need to import and instantiate the DynamoDBProvider class. The DynamoDB Provider does not have any high-level functions, as it needs to know the name of the DynamoDB table containing the parameters. DynamoDB table structure When using the default options, if you want to retrieve only single parameters, your table should be structured as such, assuming a parameter named my-parameter with a value of my-value . The id attribute should be the partition key for that table. id value my-parameter my-value With this table, when you do a dynamodb_provider.get(\"my-param\") call, this will return my-value . dynamodb.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve a value from DynamoDB value = dynamodb_provider . get ( \"my-parameter\" ) Retrieve multiple values If you want to be able to retrieve multiple parameters at once sharing the same id , your table needs to contain a sort key name sk . For example, if you want to retrieve multiple parameters having my-hash-key as ID: id sk value my-hash-key param-a my-value-a my-hash-key param-b my-value-b my-hash-key param-c my-value-c With this table, when you do a dynamodb_provider.get_multiple(\"my-hash-key\") call, you will receive the following dict as a response: 1 2 3 4 5 { \"param-a\": \"my-value-a\", \"param-b\": \"my-value-b\", \"param-c\": \"my-value-c\" } Example: dynamodb_multiple.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" ) def handler ( event , context ): # Retrieve multiple values by performing a Query on the DynamoDB table # This returns a dict with the sort key attribute as dict key. values = dynamodb_provider . get_multiple ( \"my-hash-key\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) Additional arguments The Amazon DynamoDB provider supports four additional arguments at initialization: Parameter Mandatory Default Description table_name Yes (N/A) Name of the DynamoDB table containing the parameter values. key_attr No id Hash key for the DynamoDB table. sort_attr No sk Range key for the DynamoDB table. You don't need to set this if you don't use the get_multiple() method. value_attr No value Name of the attribute containing the parameter value. dynamodb.py 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities import parameters dynamodb_provider = parameters . DynamoDBProvider ( table_name = \"my-table\" , key_attr = \"MyKeyAttr\" , sort_attr = \"MySortAttr\" , value_attr = \"MyvalueAttr\" ) def handler ( event , context ): value = dynamodb_provider . get ( \"my-parameter\" )","title":"DynamoDB"},{"location":"utilities/parameters/#app-config","text":"New in 1.10.0 For configurations stored in App Config, use get_app_config . The following will retrieve the latest version and store it in the cache. appconfig.py 1 2 3 4 5 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): # Retrieve a single configuration, latest version value : bytes = parameters . get_app_config ( name = \"my_configuration\" , environment = \"my_env\" , application = \"my_app\" )","title":"App Config"},{"location":"utilities/parameters/#appconfigprovider-class","text":"Alternatively, you can use the AppConfigProvider class, which give more flexibility, such as the ability to configure the underlying SDK client. This can be used to retrieve values from other regions, change the retry behavior, etc. appconfig.py 1 2 3 4 5 6 7 8 9 from aws_lambda_powertools.utilities import parameters from botocore.config import Config config = Config ( region_name = \"us-west-1\" ) appconf_provider = parameters . AppConfigProvider ( environment = \"my_env\" , application = \"my_app\" , config = config ) def handler ( event , context ): # Retrieve a single secret value : bytes = appconf_provider . get ( \"my_conf\" )","title":"AppConfigProvider class"},{"location":"utilities/parameters/#create-your-own-provider","text":"You can create your own custom parameter store provider by inheriting the BaseProvider class, and implementing both _get() and _get_multiple() methods to retrieve a single, or multiple parameters from your custom store. All transformation and caching logic is handled by the get() and get_multiple() methods from the base provider class. Here is an example implementation using S3 as a custom parameter store: custom_provider.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 import copy from aws_lambda_powertools.utilities import BaseProvider import boto3 class S3Provider ( BaseProvider ): bucket_name = None client = None def __init__ ( self , bucket_name : str ): # Initialize the client to your custom parameter store # E.g.: self . bucket_name = bucket_name self . client = boto3 . client ( \"s3\" ) def _get ( self , name : str , ** sdk_options ) -> str : # Retrieve a single value # E.g.: sdk_options [ \"Bucket\" ] = self . bucket_name sdk_options [ \"Key\" ] = name response = self . client . get_object ( ** sdk_options ) return def _get_multiple ( self , path : str , ** sdk_options ) -> Dict [ str , str ]: # Retrieve multiple values # E.g.: list_sdk_options = copy . deepcopy ( sdk_options ) list_sdk_options [ \"Bucket\" ] = self . bucket_name list_sdk_options [ \"Prefix\" ] = path list_response = self . client . list_objects_v2 ( ** list_sdk_options ) parameters = {} for obj in list_response . get ( \"Contents\" , []): get_sdk_options = copy . deepcopy ( sdk_options ) get_sdk_options [ \"Bucket\" ] = self . bucket_name get_sdk_options [ \"Key\" ] = obj [ \"Key\" ] get_response = self . client . get_object ( ** get_sdk_options ) parameters [ obj [ \"Key\" ]] = get_response [ \"Body\" ] . read () . decode () return parameters","title":"Create your own provider"},{"location":"utilities/parameters/#transform-values","text":"For parameters stored in JSON or Base64 format, you can use the transform argument for deserialization - The transform argument is available across all providers, including the high level functions. transform.py 1 2 3 4 5 6 7 8 9 10 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # Transform a JSON string value_from_json = ssm_provider . get ( \"/my/json/parameter\" , transform = \"json\" ) # Transform a Base64 encoded string value_from_binary = ssm_provider . get ( \"/my/binary/parameter\" , transform = \"binary\" ) You can also use the transform argument with high-level functions: transform.py 1 2 3 4 from aws_lambda_powertools.utilities import parameters def handler ( event , context ): value_from_json = parameters . get_parameter ( \"/my/json/parameter\" , transform = \"json\" )","title":"Transform values"},{"location":"utilities/parameters/#partial-transform-failures-with-get_multiple","text":"If you use transform with get_multiple() , you can have a single malformed parameter value. To prevent failing the entire request, the method will return a None value for the parameters that failed to transform. You can override this by setting the raise_on_transform_error argument to True . If you do so, a single transform error will raise a TransformParameterError exception. For example, if you have three parameters ( /param/a , /param/b and /param/c ) but /param/c is malformed: partial_failures.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from aws_lambda_powertools.utilities import parameters ssm_provider = parameters . SSMProvider () def handler ( event , context ): # This will display: # /param/a: [some value] # /param/b: [some value] # /param/c: None values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" ) for k , v in values . items (): print ( f \" { k } : { v } \" ) # This will raise a TransformParameterError exception values = ssm_provider . get_multiple ( \"/param\" , transform = \"json\" , raise_on_transform_error = True )","title":"Partial transform failures with get_multiple()"},{"location":"utilities/parameters/#additional-sdk-arguments","text":"You can use arbitrary keyword arguments to pass it directly to the underlying SDK method. ssm_parameter_store.py 1 2 3 4 5 6 7 from aws_lambda_powertools.utilities import parameters secrets_provider = parameters . SecretsProvider () def handler ( event , context ): # The 'VersionId' argument will be passed to the underlying get_secret_value() call. value = secrets_provider . get ( \"my-secret\" , VersionId = \"e62ec170-6b01-48c7-94f3-d7497851a8d2\" ) Here is the mapping between this utility's functions and methods and the underlying SDK: Provider Function/Method Client name Function name SSM Parameter Store get_parameter ssm get_parameter SSM Parameter Store get_parameters ssm get_parameters_by_path SSM Parameter Store SSMProvider.get ssm get_parameter SSM Parameter Store SSMProvider.get_multiple ssm get_parameters_by_path Secrets Manager get_secret secretsmanager get_secret_value Secrets Manager SecretsManager.get secretsmanager get_secret_value DynamoDB DynamoDBProvider.get dynamodb ( Table resource ) DynamoDB DynamoDBProvider.get_multiple dynamodb ( Table resource ) App Config get_app_config appconfig get_configuration","title":"Additional SDK arguments"},{"location":"utilities/parser/","text":"Warning It requires an extra dependency before using it. This utility provides data parsing and deep validation using Pydantic . Key features Defines data in pure Python classes, then parse, validate and extract only what you want Built-in envelopes to unwrap, extend, and validate popular event sources payloads Enforces type hints at runtime with user-friendly errors Extra dependency Info This will install pydantic and typing_extensions Install parser's extra dependencies using pip install aws-lambda-powertools[pydantic] . Defining models You can define models to parse incoming events by inheriting from BaseModel . hello_world_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime. Parsing events You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input. event_parser decorator Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. NOTE: This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . event_parser_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel , ValidationError from aws_lambda_powertools.utilities.typing import LambdaContext import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ items for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string parse function Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. parse_standalone_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" } Built-in models Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service extending built-in models You can extend them to include your own models, and yet have all other known fields parsed along the way. EventBridge example extending_builtin_models.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel Envelopes When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. parse_eventbridge_payload.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeModel , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeModel ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeModel as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel built-in envelopes Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model] bringing your own envelope You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model eventbridge_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope eventbridge_envelope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model Data model validation Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant validating fields Quick validation to verify whether the field message has the value of hello world . deep_data_validation.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: 1 2 message Message must be hello world! (type=value_error) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" }) validating entire model root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation . Advanced use cases Info Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. serializing_models_exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc. FAQ When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: escape_hatch.py 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"Parser"},{"location":"utilities/parser/#defining-models","text":"You can define models to parse incoming events by inheriting from BaseModel . hello_world_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.parser import BaseModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing These are simply Python classes that inherit from BaseModel. Parser enforces type hints declared in your model at runtime.","title":"Defining models"},{"location":"utilities/parser/#parsing-events","text":"You can parse inbound events using event_parser decorator, or the standalone parse function. Both are also able to parse either dictionary or JSON string as an input.","title":"Parsing events"},{"location":"utilities/parser/#event_parser-decorator","text":"Use the decorator for fail fast scenarios where you want your Lambda function to raise an exception in the event of a malformed payload. event_parser decorator will throw a ValidationError if your event cannot be parsed according to the model. NOTE: This decorator will replace the event object with the parsed model if successful . This means you might be careful when nesting other decorators that expect event to be a dict . event_parser_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 from aws_lambda_powertools.utilities.parser import event_parser , BaseModel , ValidationError from aws_lambda_powertools.utilities.typing import LambdaContext import json class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing @event_parser ( model = Order ) def handler ( event : Order , context : LambdaContext ): print ( event . id ) print ( event . description ) print ( event . items ) order_items = [ items for item in event . items ] ... payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } handler ( event = payload , context = LambdaContext ()) handler ( event = json . dumps ( payload ), context = LambdaContext ()) # also works if event is a JSON string","title":"event_parser decorator"},{"location":"utilities/parser/#parse-function","text":"Use this standalone function when you want more control over the data validation process, for example returning a 400 error for malformed payloads. parse_standalone_example.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] # nesting models are supported optional_field : Optional [ str ] # this field may or may not be available when parsing payload = { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { # this will cause a validation error \"id\" : [ 1015938732 ], \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } def my_function (): try : parsed_payload : Order = parse ( event = payload , model = Order ) # payload dict is now parsed into our model return parsed_payload . items except ValidationError : return { \"status_code\" : 400 , \"message\" : \"Invalid order\" }","title":"parse function"},{"location":"utilities/parser/#built-in-models","text":"Parser comes with the following built-in models: Model name Description DynamoDBStreamModel Lambda Event Source payload for Amazon DynamoDB Streams EventBridgeModel Lambda Event Source payload for Amazon EventBridge SqsModel Lambda Event Source payload for Amazon SQS AlbModel Lambda Event Source payload for Amazon Application Load Balancer CloudwatchLogsModel Lambda Event Source payload for Amazon CloudWatch Logs S3Model Lambda Event Source payload for Amazon S3 KinesisDataStreamModel Lambda Event Source payload for Amazon Kinesis Data Streams SesModel Lambda Event Source payload for Amazon Simple Email Service SnsModel Lambda Event Source payload for Amazon Simple Notification Service","title":"Built-in models"},{"location":"utilities/parser/#extending-built-in-models","text":"You can extend them to include your own models, and yet have all other known fields parsed along the way. EventBridge example extending_builtin_models.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 from aws_lambda_powertools.utilities.parser import parse , BaseModel from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import List , Optional class OrderItem ( BaseModel ): id : int quantity : int description : str class Order ( BaseModel ): id : int description : str items : List [ OrderItem ] class OrderEventModel ( EventBridgeModel ): detail : Order payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"OrderPurchased\" , \"source\" : \"OrderService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional\" ], \"detail\" : { \"id\" : 10876546789 , \"description\" : \"My order\" , \"items\" : [ { \"id\" : 1015938732 , \"quantity\" : 1 , \"description\" : \"item xpto\" } ] } } ret = parse ( model = OrderEventModel , event = payload ) assert ret . source == \"OrderService\" assert ret . detail . description == \"My order\" assert ret . detail_type == \"OrderPurchased\" # we rename it to snake_case since detail-type is an invalid name for order_item in ret . detail . items : ... What's going on here, you might ask : We imported our built-in model EventBridgeModel from the parser utility Defined how our Order should look like Defined how part of our EventBridge event should look like by overriding detail key within our OrderEventModel Parser parsed the original event against OrderEventModel","title":"extending built-in models"},{"location":"utilities/parser/#envelopes","text":"When trying to parse your payloads wrapped in a known structure, you might encounter the following situations: Your actual payload is wrapped around a known structure, for example Lambda Event Sources like EventBridge You're only interested in a portion of the payload, for example parsing the detail of custom events in EventBridge, or body of SQS records You can either solve these situations by creating a model of these known structures, parsing them, then extracting and parsing a key where your payload is. This can become difficult quite quickly. Parser makes this problem easier through a feature named Envelope . Envelopes can be used via envelope parameter available in both parse function and event_parser decorator. Here's an example of parsing a model found in an event coming from EventBridge, where all you want is what's inside the detail key. parse_eventbridge_payload.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from aws_lambda_powertools.utilities.parser import event_parser , parse , BaseModel , envelopes from aws_lambda_powertools.utilities.typing import LambdaContext class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"version\" : \"0\" , \"id\" : \"6a7e8feb-b491-4cf7-a9f1-bf3703467718\" , \"detail-type\" : \"CustomerSignedUp\" , \"source\" : \"CustomerService\" , \"account\" : \"111122223333\" , \"time\" : \"2020-10-22T18:43:48Z\" , \"region\" : \"us-west-1\" , \"resources\" : [ \"some_additional_\" ], \"detail\" : { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } } ret = parse ( model = UserModel , envelope = envelopes . EventBridgeModel , event = payload ) # Parsed model only contains our actual model, not the entire EventBridge + Payload parsed assert ret . password1 == ret . password2 # Same behaviour but using our decorator @event_parser ( model = UserModel , envelope = envelopes . EventBridgeModel ) def handler ( event : UserModel , context : LambdaContext ): assert event . password1 == event . password2 What's going on here, you might ask : We imported built-in envelopes from the parser utility Used envelopes.EventBridgeModel as the envelope for our UserModel model Parser parsed the original event against the EventBridge model Parser then parsed the detail key using UserModel","title":"Envelopes"},{"location":"utilities/parser/#built-in-envelopes","text":"Parser comes with the following built-in envelopes, where Model in the return section is your given model. Envelope name Behaviour Return DynamoDBStreamEnvelope 1. Parses data using DynamoDBStreamModel . 2. Parses records in NewImage and OldImage keys using your model. 3. Returns a list with a dictionary containing NewImage and OldImage keys List[Dict[str, Optional[Model]]] EventBridgeEnvelope 1. Parses data using EventBridgeModel . 2. Parses detail key using your model and returns it. Model SqsEnvelope 1. Parses data using SqsModel . 2. Parses records in body key using your model and return them in a list. List[Model] CloudWatchLogsEnvelope 1. Parses data using CloudwatchLogsModel which will base64 decode and decompress it. 2. Parses records in message key using your model and return them in a list. List[Model] KinesisDataStreamEnvelope 1. Parses data using KinesisDataStreamModel which will base64 decode it. 2. Parses records in in Records key using your model and returns them in a list. List[Model] SnsEnvelope 1. Parses data using SnsModel . 2. Parses records in body key using your model and return them in a list. List[Model] SnsSqsEnvelope 1. Parses data using SqsModel . 2. Parses SNS records in body key using SnsNotificationModel . 3. Parses data in Message key using your model and return them in a list. List[Model]","title":"built-in envelopes"},{"location":"utilities/parser/#bringing-your-own-envelope","text":"You can create your own Envelope model and logic by inheriting from BaseEnvelope , and implementing the parse method. Here's a snippet of how the EventBridge envelope we demonstrated previously is implemented. EventBridge Model eventbridge_model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from datetime import datetime from typing import Any , Dict , List from aws_lambda_powertools.utilities.parser import BaseModel , Field class EventBridgeModel ( BaseModel ): version : str id : str # noqa: A003,VNE003 source : str account : str time : datetime region : str resources : List [ str ] detail_type : str = Field ( None , alias = \"detail-type\" ) detail : Dict [ str , Any ] EventBridge Envelope eventbridge_envelope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 from aws_lambda_powertools.utilities.parser import BaseEnvelope , models from aws_lambda_powertools.utilities.parser.models import EventBridgeModel from typing import Any , Dict , Optional , TypeVar Model = TypeVar ( \"Model\" , bound = BaseModel ) class EventBridgeEnvelope ( BaseEnvelope ): def parse ( self , data : Optional [ Union [ Dict [ str , Any ], Any ]], model : Model ) -> Optional [ Model ]: \"\"\"Parses data found with model provided Parameters ---------- data : Dict Lambda event to be parsed model : Model Data model provided to parse after extracting data using envelope Returns ------- Any Parsed detail payload with model provided \"\"\" parsed_envelope = EventBridgeModel . parse_obj ( data ) return self . _parse ( data = parsed_envelope . detail , model = model ) What's going on here, you might ask : We defined an envelope named EventBridgeEnvelope inheriting from BaseEnvelope Implemented the parse abstract method taking data and model as parameters Then, we parsed the incoming data with our envelope to confirm it matches EventBridge's structure defined in EventBridgeModel Lastly, we call _parse from BaseEnvelope to parse the data in our envelope (.detail) using the customer model","title":"bringing your own envelope"},{"location":"utilities/parser/#data-model-validation","text":"Warning This is radically different from the Validator utility which validates events against JSON Schema. You can use parser's validator for deep inspection of object values and complex relationships. There are two types of class method decorators you can use: validator - Useful to quickly validate an individual field and its value root_validator - Useful to validate the entire model's data Keep the following in mind regardless of which decorator you end up using it: You must raise either ValueError , TypeError , or AssertionError when value is not compliant You must return the value(s) itself if compliant","title":"Data model validation"},{"location":"utilities/parser/#validating-fields","text":"Quick validation to verify whether the field message has the value of hello world . deep_data_validation.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str @validator ( 'message' ) def is_hello_world ( cls , v ): if v != \"hello world\" : raise ValueError ( \"Message must be hello world!\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" }) If you run as-is, you should expect the following error with the message we provided in our exception: 1 2 message Message must be hello world! (type=value_error) Alternatively, you can pass '*' as an argument for the decorator so that you can validate every value available. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class HelloWorldModel ( BaseModel ): message : str sender : str @validator ( '*' ) def has_whitespace ( cls , v ): if ' ' not in v : raise ValueError ( \"Must have whitespace...\" ) return v parse ( model = HelloWorldModel , event = { \"message\" : \"hello universe\" , \"sender\" : \"universe\" })","title":"validating fields"},{"location":"utilities/parser/#validating-entire-model","text":"root_validator can help when you have a complex validation mechanism. For example finding whether data has been omitted, comparing field values, etc. validate_all_field_values.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from aws_lambda_powertools.utilities.parser import parse , BaseModel , validator class UserModel ( BaseModel ): username : str password1 : str password2 : str @root_validator def check_passwords_match ( cls , values ): pw1 , pw2 = values . get ( 'password1' ), values . get ( 'password2' ) if pw1 is not None and pw2 is not None and pw1 != pw2 : raise ValueError ( 'passwords do not match' ) return values payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } parse ( model = UserModel , event = payload ) Info You can read more about validating list items, reusing validators, validating raw inputs, and a lot more in Pydantic's documentation .","title":"validating entire model"},{"location":"utilities/parser/#advanced-use-cases","text":"Info Looking to auto-generate models from JSON, YAML, JSON Schemas, OpenApi, etc? Use Koudai Aono's data model code generation tool for Pydantic There are number of advanced use cases well documented in Pydantic's doc such as creating immutable models , declaring fields with dynamic values e.g. UUID, and helper functions to parse models from files, str , etc. Two possible unknown use cases are Models and exception' serialization. Models have methods to export them as dict , JSON , JSON Schema , and Validation exceptions can be exported as JSON. serializing_models_exceptions.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from aws_lambda_powertools.utilities import Logger from aws_lambda_powertools.utilities.parser import parse , BaseModel , ValidationError , validator logger = Logger ( service = \"user\" ) class UserModel ( BaseModel ): username : str password1 : str password2 : str payload = { \"username\" : \"universe\" , \"password1\" : \"myp@ssword\" , \"password2\" : \"repeat password\" } def my_function (): try : return parse ( model = UserModel , event = payload ) except ValidationError as e : logger . exception ( e . json ()) return { \"status_code\" : 400 , \"message\" : \"Invalid username\" } User : UserModel = my_function () user_dict = User . dict () user_json = User . json () user_json_schema_as_dict = User . schema () user_json_schema_as_json = User . schema_json ( indent = 2 ) These can be quite useful when manipulating models that later need to be serialized as inputs for services like DynamoDB, EventBridge, etc.","title":"Advanced use cases"},{"location":"utilities/parser/#faq","text":"When should I use parser vs data_classes utility? Use data classes utility when you're after autocomplete, self-documented attributes and helpers to extract data from common event sources. Parser is best suited for those looking for a trade-off between defining their models for deep validation, parsing and autocomplete for an additional dependency to be brought in. How do I import X from Pydantic? We export most common classes, exceptions, and utilities from Pydantic as part of parser e.g. from aws_lambda_powertools.utilities.parser import BaseModel . If what's your trying to use isn't available as part of the high level import system, use the following escape hatch mechanism: escape_hatch.py 1 from aws_lambda_powertools.utilities.parser.pydantic import < what you 'd like to import' > What is the cold start impact in bringing this additional dependency? No significant cold start impact. It does increase the final uncompressed package by 71M , when you bring the additional dependency that parser requires. Artillery load test sample against a hello world sample using Tracer, Metrics, and Logger with and without parser. No parser Uncompressed package size : 55M, p99 : 180.3ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:36:07(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 114.81 Response time (msec): min: 54.9 max: 1684.9 median: 68 p95: 109.1 p99: 180.3 Scenario counts: 0: 10 (100%) Codes: 200: 2000 With parser Uncompressed package size : 128M, p99 : 193.1ms 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Summary report @ 14:29:23(+0200) 2020-10-23 Scenarios launched: 10 Scenarios completed: 10 Requests completed: 2000 Mean response/sec: 111.67 Response time (msec): min: 54.3 max: 1887.2 median: 66.1 p95: 113.3 p99: 193.1 Scenario counts: 0: 10 (100%) Codes: 200: 2000","title":"FAQ"},{"location":"utilities/typing/","text":"This typing utility provides static typing classes that can be used to ease the development by providing the IDE type hints. LambdaContext The LambdaContext typing is typically used in the handler method for the Lambda function. index.py 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"Typing"},{"location":"utilities/typing/#lambdacontext","text":"The LambdaContext typing is typically used in the handler method for the Lambda function. index.py 1 2 3 4 5 6 from typing import Any , Dict from aws_lambda_powertools.utilities.typing import LambdaContext def handler ( event : Dict [ str , Any ], context : LambdaContext ) -> Dict [ str , Any ]: # Insert business logic return event","title":"LambdaContext"},{"location":"utilities/validation/","text":"This utility provides JSON Schema validation for events and responses, including JMESPath support to unwrap events before validation. Key features Validate incoming event and response JMESPath support to unwrap events before validation applies Built-in envelopes to unwrap popular event sources payloads Validating events You can validate inbound and outbound events using validator decorator. You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename. Validator decorator Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator json_schema_dict = { .. } response_json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , outbound_schema = response_json_schema_dict ) def handler ( event , context ): return event Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both. Validate function Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError json_schema_dict = { .. } def handler ( event , context ): try : validate ( event = event , schema = json_schema_dict ) except SchemaValidationError as e : # do something before re-raising raise return event Validating custom formats New in 1.10.0 JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. If you have JSON Schemas with custom formats, for example having a int64 for high precision integers, you can pass an optional validation to handle each type using formats parameter - Otherwise it'll fail validation: Example of custom integer format 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.validation import validate event = {} # some event schema_with_custom_format = {} # some JSON schema that defines a custom format custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schema_with_custom_format , formats = custom_format ) Unwrapping events prior to validation You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } Here is how you'd use the envelope parameter to extract the payload inside the detail key before validating: unwrapping_events.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator , validate json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = \"detail\" ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"detail\" ) return event This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload. Built-in envelopes This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import envelopes , validate , validator json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) return event Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data) Built-in JMESPath functions You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc. powertools_json function Use powertools_json function to decode any JSON String. This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(data)\" ) return event handler ( event = sample_event , context = {}) powertools_base64 function Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(powertools_base64(data))\" ) return event handler ( event = sample_event , context = {}) powertools_base64_gzip function Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) return event handler ( event = sample_event , context = {}) Bring your own JMESPath function Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. This will replace all provided built-in functions such as powertools_json , so you will no longer be able to use them . For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.validation import validate from jmespath import functions json_schema_dict = { .. } class CustomFunctions ( functions . Functions ): @functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"\" , jmespath_options =** custom_jmespath_options ) return event","title":"Validation"},{"location":"utilities/validation/#validating-events","text":"You can validate inbound and outbound events using validator decorator. You can also use the standalone validate function, if you want more control over the validation process such as handling a validation error. We support any JSONSchema draft supported by fastjsonschema library. Warning Both validator decorator and validate standalone function expects your JSON Schema to be a dictionary , not a filename.","title":"Validating events"},{"location":"utilities/validation/#validator-decorator","text":"Validator decorator is typically used to validate either inbound or functions' response. It will fail fast with SchemaValidationError exception if event or response doesn't conform with given JSON Schema. validator_decorator.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator json_schema_dict = { .. } response_json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , outbound_schema = response_json_schema_dict ) def handler ( event , context ): return event Note It's not a requirement to validate both inbound and outbound schemas - You can either use one, or both.","title":"Validator decorator"},{"location":"utilities/validation/#validate-function","text":"Validate standalone function is typically used within the Lambda handler, or any other methods that perform data validation. You can also gracefully handle schema validation errors by catching SchemaValidationError exception. validator_decorator.py 1 2 3 4 5 6 7 8 9 10 11 12 13 from aws_lambda_powertools.utilities.validation import validate from aws_lambda_powertools.utilities.validation.exceptions import SchemaValidationError json_schema_dict = { .. } def handler ( event , context ): try : validate ( event = event , schema = json_schema_dict ) except SchemaValidationError as e : # do something before re-raising raise return event","title":"Validate function"},{"location":"utilities/validation/#validating-custom-formats","text":"New in 1.10.0 JSON Schema DRAFT 7 has many new built-in formats such as date, time, and specifically a regex format which might be a better replacement for a custom format, if you do have control over the schema. If you have JSON Schemas with custom formats, for example having a int64 for high precision integers, you can pass an optional validation to handle each type using formats parameter - Otherwise it'll fail validation: Example of custom integer format 1 2 3 4 5 6 { \"lastModifiedTime\" : { \"format\" : \"int64\" , \"type\" : \"integer\" } } For each format defined in a dictionary key, you must use a regex, or a function that returns a boolean to instruct the validator on how to proceed when encountering that type. 1 2 3 4 5 6 7 8 9 10 11 from aws_lambda_powertools.utilities.validation import validate event = {} # some event schema_with_custom_format = {} # some JSON schema that defines a custom format custom_format = { \"int64\" : True , # simply ignore it, \"positive\" : lambda x : False if x < 0 else True } validate ( event = event , schema = schema_with_custom_format , formats = custom_format )","title":"Validating custom formats"},{"location":"utilities/validation/#unwrapping-events-prior-to-validation","text":"You might want to validate only a portion of your event - This is where the envelope parameter is for. Envelopes are JMESPath expressions to extract a portion of JSON you want before applying JSON Schema validation. Here is a sample custom EventBridge event, where we only validate what's inside the detail key: sample_wrapped_event.json 1 2 3 4 5 6 7 8 9 10 { \"id\" : \"cdc73f9d-aea9-11e3-9d5a-835b769c0d9c\" , \"detail-type\" : \"Scheduled Event\" , \"source\" : \"aws.events\" , \"account\" : \"123456789012\" , \"time\" : \"1970-01-01T00:00:00Z\" , \"region\" : \"us-east-1\" , \"resources\" : [ \"arn:aws:events:us-east-1:123456789012:rule/ExampleRule\" ], \"detail\" : { \"message\" : \"hello hello\" , \"username\" : \"blah blah\" } } Here is how you'd use the envelope parameter to extract the payload inside the detail key before validating: unwrapping_events.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import validator , validate json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = \"detail\" ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"detail\" ) return event This is quite powerful because you can use JMESPath Query language to extract records from arrays, slice and dice , to pipe expressions and function expressions , where you'd extract what you need before validating the actual payload.","title":"Unwrapping events prior to validation"},{"location":"utilities/validation/#built-in-envelopes","text":"This utility comes with built-in envelopes to easily extract the payload from popular event sources. unwrapping_popular_event_sources.py 1 2 3 4 5 6 7 8 from aws_lambda_powertools.utilities.validation import envelopes , validate , validator json_schema_dict = { .. } @validator ( inbound_schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = envelopes . EVENTBRIDGE ) return event Here is a handy table with built-in envelopes along with their JMESPath expressions in case you want to build your own. Envelope name JMESPath expression API_GATEWAY_REST \"powertools_json(body)\" API_GATEWAY_HTTP \"powertools_json(body)\" SQS \"Records[*].powertools_json(body)\" SNS \"Records[0].Sns.Message EVENTBRIDGE \"detail\" CLOUDWATCH_EVENTS_SCHEDULED \"detail\" KINESIS_DATA_STREAM \"Records[*].kinesis.powertools_json(powertools_base64(data))\" CLOUDWATCH_LOGS \"awslogs.powertools_base64_gzip(data)","title":"Built-in envelopes"},{"location":"utilities/validation/#built-in-jmespath-functions","text":"You might have events or responses that contain non-encoded JSON, where you need to decode before validating them. You can use our built-in JMESPath functions within your expressions to do exactly that to decode JSON Strings, base64, and uncompress gzip data. Info We use these for built-in envelopes to easily to decode and unwrap events from sources like Kinesis, CloudWatch Logs, etc.","title":"Built-in JMESPath functions"},{"location":"utilities/validation/#powertools_json-function","text":"Use powertools_json function to decode any JSON String. This sample will decode the value within the data key into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { 'data' : '{\"payload\": {\"message\": \"hello hello\", \"username\": \"blah blah\"}}' } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(data)\" ) return event handler ( event = sample_event , context = {})","title":"powertools_json function"},{"location":"utilities/validation/#powertools_base64-function","text":"Use powertools_base64 function to decode any base64 data. This sample will decode the base64 value within the data key, and decode the JSON string into a valid JSON before we can validate it. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"eyJtZXNzYWdlIjogImhlbGxvIGhlbGxvIiwgInVzZXJuYW1lIjogImJsYWggYmxhaCJ9=\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_json(powertools_base64(data))\" ) return event handler ( event = sample_event , context = {})","title":"powertools_base64 function"},{"location":"utilities/validation/#powertools_base64_gzip-function","text":"Use powertools_base64_gzip function to decompress and decode base64 data. This sample will decompress and decode base64 data, then use JMESPath pipeline expression to pass the result for decoding its JSON string. powertools_json_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 from aws_lambda_powertools.utilities.validation import validate json_schema_dict = { .. } sample_event = { \"data\" : \"H4sIACZAXl8C/52PzUrEMBhFX2UILpX8tPbHXWHqIOiq3Q1F0ubrWEiakqTWofTdTYYB0YWL2d5zvnuTFellBIOedoiyKH5M0iwnlKH7HZL6dDB6ngLDfLFYctUKjie9gHFaS/sAX1xNEq525QxwFXRGGMEkx4Th491rUZdV3YiIZ6Ljfd+lfSyAtZloacQgAkqSJCGhxM6t7cwwuUGPz4N0YKyvO6I9WDeMPMSo8Z4Ca/kJ6vMEYW5f1MX7W1lVxaG8vqX8hNFdjlc0iCBBSF4ERT/3Pl7RbMGMXF2KZMh/C+gDpNS7RRsp0OaRGzx0/t8e0jgmcczyLCWEePhni/23JWalzjdu0a3ZvgEaNLXeugEAAA==\" } def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"powertools_base64_gzip(data) | powertools_json(@)\" ) return event handler ( event = sample_event , context = {})","title":"powertools_base64_gzip function"},{"location":"utilities/validation/#bring-your-own-jmespath-function","text":"Warning This should only be used for advanced use cases where you have special formats not covered by the built-in functions. This will replace all provided built-in functions such as powertools_json , so you will no longer be able to use them . For special binary formats that you want to decode before applying JSON Schema validation, you can bring your own JMESPath function and any additional option via jmespath_options param. custom_jmespath_function.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from aws_lambda_powertools.utilities.validation import validate from jmespath import functions json_schema_dict = { .. } class CustomFunctions ( functions . Functions ): @functions . signature ({ 'types' : [ 'string' ]}) def _func_special_decoder ( self , s ): return my_custom_decoder_logic ( s ) custom_jmespath_options = { \"custom_functions\" : CustomFunctions ()} def handler ( event , context ): validate ( event = event , schema = json_schema_dict , envelope = \"\" , jmespath_options =** custom_jmespath_options ) return event","title":"Bring your own JMESPath function"}]}